#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gauzæ–‡æ¡£Agent - FastAPIæœåŠ¡å™¨
å°†å¤šAgentæ–‡æ¡£ç”Ÿæˆç³»ç»Ÿå°è£…ä¸ºRESTful APIæœåŠ¡

æä¾›çš„æ¥å£ï¼š
- POST /generate_document - ç”Ÿæˆæ–‡æ¡£ï¼ˆè‡ªåŠ¨ç®¡ç†è¾“å‡ºç›®å½•ï¼‰
- GET /health - å¥åº·æ£€æŸ¥
- GET /status - ç³»ç»ŸçŠ¶æ€
- POST /set_concurrency - è®¾ç½®å¹¶å‘å‚æ•°
- GET /download/{file_id} - ä¸‹è½½ç”Ÿæˆçš„æ–‡ä»¶ï¼ˆå¤‡ç”¨ï¼‰
- MinIOè‡ªåŠ¨ä¸Šä¼  - ä¸»è¦æ–‡ä»¶åˆ†å‘æ–¹å¼
"""

import sys
import os
import uuid
import asyncio
import logging
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path

# å¿…é¡»åœ¨æ‰€æœ‰å…¶ä»–å¯¼å…¥ä¹‹å‰ç¦ç”¨ChromaDB telemetry
os.environ['ANONYMIZED_TELEMETRY'] = 'False'
os.environ['CHROMA_TELEMETRY_DISABLED'] = 'True'

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from fastapi import FastAPI, HTTPException, BackgroundTasks, status
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn
import json

# å¯¼å…¥ä¸»è¦ç»„ä»¶
try:
    from main import DocumentGenerationPipeline
    from config.settings import setup_logging, get_config
    from config.minio_config import get_minio_client, upload_document_files
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

# è®¾ç½®æ—¥å¿—
setup_logging()
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="Gauzæ–‡æ¡£Agent API",
    description="åŸºäºå¤šAgentæ¶æ„çš„æ™ºèƒ½é•¿æ–‡æ¡£ç”Ÿæˆç³»ç»ŸAPIæœåŠ¡",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡
pipeline: Optional[DocumentGenerationPipeline] = None
generation_tasks: Dict[str, Dict[str, Any]] = {}  # å­˜å‚¨ä»»åŠ¡çŠ¶æ€
file_storage: Dict[str, str] = {}  # å­˜å‚¨æ–‡ä»¶æ˜ å°„

# ===== æ—¥å¿—ç®¡ç†å™¨ =====

class LogManager:
    """ä»»åŠ¡æ—¥å¿—ç®¡ç†å™¨"""
    def __init__(self):
        self.task_logs: Dict[str, List[Dict[str, Any]]] = {}  # å­˜å‚¨ä»»åŠ¡æ—¥å¿—
        self.log_subscribers: Dict[str, List[asyncio.Queue]] = {}  # å­˜å‚¨æ—¥å¿—è®¢é˜…è€…
        self.max_logs_per_task = 1000  # æ¯ä¸ªä»»åŠ¡æœ€å¤šä¿å­˜çš„æ—¥å¿—æ•°é‡
        
    def add_log(self, task_id: str, log_entry: Dict[str, Any]):
        """æ·»åŠ æ—¥å¿—æ¡ç›®"""
        if task_id not in self.task_logs:
            self.task_logs[task_id] = []
        
        # æ·»åŠ æ—¶é—´æˆ³ï¼ˆå¦‚æœæ²¡æœ‰çš„è¯ï¼‰
        if 'timestamp' not in log_entry:
            log_entry['timestamp'] = datetime.now().isoformat()
            
        self.task_logs[task_id].append(log_entry)
        
        # é™åˆ¶æ—¥å¿—æ•°é‡ï¼Œé¿å…å†…å­˜æº¢å‡º
        if len(self.task_logs[task_id]) > self.max_logs_per_task:
            self.task_logs[task_id] = self.task_logs[task_id][-self.max_logs_per_task:]
        
        # æ¨é€ç»™æ‰€æœ‰è®¢é˜…è€…
        self._notify_subscribers(task_id, log_entry)
        
        # åŒæ—¶è®°å½•åˆ°ç³»ç»Ÿæ—¥å¿—
        log_level = log_entry.get('type', 'info')
        message = f"[{task_id}] {log_entry.get('message', '')}"
        if log_level == 'error':
            logger.error(message)
        elif log_level == 'warning':
            logger.warning(message)
        else:
            logger.info(message)
    
    def _notify_subscribers(self, task_id: str, log_entry: Dict[str, Any]):
        """é€šçŸ¥è®¢é˜…è€…"""
        if task_id in self.log_subscribers:
            # åˆ›å»ºé˜Ÿåˆ—å‰¯æœ¬ä»¥é¿å…è¿­ä»£æ—¶ä¿®æ”¹
            subscribers = self.log_subscribers[task_id].copy()
            for queue in subscribers:
                try:
                    queue.put_nowait(log_entry)
                except asyncio.QueueFull:
                    # é˜Ÿåˆ—æ»¡äº†ï¼Œç§»é™¤è¿™ä¸ªè®¢é˜…è€…
                    try:
                        self.log_subscribers[task_id].remove(queue)
                    except ValueError:
                        pass  # é˜Ÿåˆ—å·²ç»è¢«ç§»é™¤
    
    async def subscribe_logs(self, task_id: str) -> asyncio.Queue:
        """è®¢é˜…ä»»åŠ¡æ—¥å¿—"""
        queue = asyncio.Queue(maxsize=100)
        if task_id not in self.log_subscribers:
            self.log_subscribers[task_id] = []
        self.log_subscribers[task_id].append(queue)
        return queue
    
    def unsubscribe_logs(self, task_id: str, queue: asyncio.Queue):
        """å–æ¶ˆè®¢é˜…ä»»åŠ¡æ—¥å¿—"""
        if task_id in self.log_subscribers:
            try:
                self.log_subscribers[task_id].remove(queue)
                if not self.log_subscribers[task_id]:  # å¦‚æœæ²¡æœ‰è®¢é˜…è€…äº†
                    del self.log_subscribers[task_id]
            except ValueError:
                pass  # é˜Ÿåˆ—ä¸åœ¨åˆ—è¡¨ä¸­
    
    def get_logs(self, task_id: str) -> List[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çš„æ‰€æœ‰æ—¥å¿—"""
        return self.task_logs.get(task_id, [])
    
    def cleanup_task_logs(self, task_id: str):
        """æ¸…ç†ä»»åŠ¡æ—¥å¿—ï¼ˆä»»åŠ¡å®Œæˆåè°ƒç”¨ï¼‰"""
        # ä¿ç•™æ—¥å¿—1å°æ—¶ï¼Œç„¶åæ¸…ç†
        if task_id in self.task_logs:
            # è¿™é‡Œå¯ä»¥å®ç°å»¶æ—¶æ¸…ç†ï¼Œæš‚æ—¶ä¿ç•™
            pass
        
        # ç«‹å³æ¸…ç†è®¢é˜…è€…
        if task_id in self.log_subscribers:
            del self.log_subscribers[task_id]

# åˆ›å»ºå…¨å±€æ—¥å¿—ç®¡ç†å™¨
log_manager = LogManager()

# ===== æ•°æ®æ¨¡å‹ =====

class DocumentGenerationRequest(BaseModel):
    """æ–‡æ¡£ç”Ÿæˆè¯·æ±‚æ¨¡å‹"""
    query: str = Field(..., description="æ–‡æ¡£ç”Ÿæˆéœ€æ±‚æè¿°", min_length=1, max_length=2000)
    project_name: str = Field(..., description="é¡¹ç›®åç§°ï¼Œç”¨äºRAGæ£€ç´¢", min_length=1, max_length=100)
    
    class Config:
        json_schema_extra = {
            "example": {
                "query": "æˆ‘æƒ³ç”Ÿæˆä¸€ä¸ªå…³äºåŒ»çµå¤åº™çš„æ–‡ç‰©å½±å“è¯„ä¼°æŠ¥å‘Š",
                "project_name": "åŒ»çµå¤åº™"
            }
        }

class ConcurrencySettings(BaseModel):
    """å¹¶å‘è®¾ç½®æ¨¡å‹"""
    orchestrator_workers: Optional[int] = Field(None, ge=1, le=10, description="ç¼–æ’ä»£ç†çº¿ç¨‹æ•°")
    react_workers: Optional[int] = Field(None, ge=1, le=10, description="æ£€ç´¢ä»£ç†çº¿ç¨‹æ•°")
    content_workers: Optional[int] = Field(None, ge=1, le=10, description="å†…å®¹ç”Ÿæˆä»£ç†çº¿ç¨‹æ•°")
    rate_delay: Optional[float] = Field(None, ge=0.1, le=10.0, description="è¯·æ±‚é—´éš”æ—¶é—´(ç§’)")
    
    class Config:
        json_schema_extra = {
            "example": {
                "orchestrator_workers": 3,
                "react_workers": 5,
                "content_workers": 4,
                "rate_delay": 1.0
            }
        }

class TaskStatus(BaseModel):
    """ä»»åŠ¡çŠ¶æ€æ¨¡å‹"""
    task_id: str
    status: str  # pending, running, completed, failed
    progress: str
    created_at: datetime
    updated_at: datetime
    request: Optional[Dict[str, Any]] = None
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class DocumentGenerationResponse(BaseModel):
    """æ–‡æ¡£ç”Ÿæˆå“åº”æ¨¡å‹"""
    task_id: str = Field(..., description="ä»»åŠ¡ID")
    status: str = Field(..., description="ä»»åŠ¡çŠ¶æ€")
    message: str = Field(..., description="å“åº”æ¶ˆæ¯")
    files: Optional[Dict[str, str]] = Field(None, description="ç”Ÿæˆçš„æ–‡ä»¶ï¼ˆæœ¬åœ°ä¸‹è½½é“¾æ¥ï¼‰")
    minio_urls: Optional[Dict[str, str]] = Field(None, description="MinIOå­˜å‚¨çš„æ–‡ä»¶ä¸‹è½½é“¾æ¥")

class SystemStatus(BaseModel):
    """ç³»ç»ŸçŠ¶æ€æ¨¡å‹"""
    service: str
    status: str
    version: str
    active_tasks: int
    total_tasks: int
    concurrency_settings: Dict[str, Any]
    uptime: str
    minio_status: str = Field(..., description="MinIOå­˜å‚¨æœåŠ¡çŠ¶æ€")

# ===== åˆå§‹åŒ–å‡½æ•° =====

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    global pipeline
    try:
        logger.info("ğŸš€ æ­£åœ¨å¯åŠ¨Gauzæ–‡æ¡£Agent APIæœåŠ¡...")
        pipeline = DocumentGenerationPipeline()
        logger.info("âœ… æ–‡æ¡£ç”Ÿæˆæµæ°´çº¿åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs("outputs", exist_ok=True)
        os.makedirs("api_outputs", exist_ok=True)
        
        # åˆå§‹åŒ–MinIOå®¢æˆ·ç«¯
        minio_client = get_minio_client()
        if minio_client.is_available():
            logger.info("âœ… MinIOå®¢æˆ·ç«¯è¿æ¥æˆåŠŸ")
        else:
            logger.warning("âš ï¸ MinIOå®¢æˆ·ç«¯è¿æ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨æœ¬åœ°æ–‡ä»¶å­˜å‚¨")
        
        logger.info("ğŸŒŸ Gauzæ–‡æ¡£Agent APIæœåŠ¡å¯åŠ¨å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        raise

@app.on_event("shutdown")
async def shutdown_event():
    """å…³é—­æ—¶æ¸…ç†èµ„æº"""
    logger.info("ğŸ”„ æ­£åœ¨å…³é—­Gauzæ–‡æ¡£Agent APIæœåŠ¡...")
    
    # æ¸…ç†æœªå®Œæˆçš„ä»»åŠ¡
    for task_id, task_info in generation_tasks.items():
        if task_info["status"] in ["pending", "running"]:
            task_info["status"] = "cancelled"
            task_info["updated_at"] = datetime.now()
    
    logger.info("âœ… æœåŠ¡å…³é—­å®Œæˆ")

# ===== æ ¸å¿ƒAPIæ¥å£ =====

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {
        "status": "healthy",
        "service": "Gauzæ–‡æ¡£Agent API",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0"
    }

@app.get("/logs/{task_id}/stream")
async def stream_task_logs(task_id: str):
    """å®æ—¶æ¨é€ä»»åŠ¡æ—¥å¿—æµï¼ˆServer-Sent Eventsï¼‰"""
    
    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
    if task_id not in generation_tasks:
        raise HTTPException(status_code=404, detail=f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
    
    async def log_generator():
        """æ—¥å¿—ç”Ÿæˆå™¨"""
        log_queue = None
        try:
            # è®¢é˜…æ—¥å¿—
            log_queue = await log_manager.subscribe_logs(task_id)
            
            # é¦–å…ˆå‘é€å†å²æ—¥å¿—
            historical_logs = log_manager.get_logs(task_id)
            for log_entry in historical_logs:
                data = json.dumps(log_entry, ensure_ascii=False)
                yield f"data: {data}\n\n"
            
            # å‘é€å½“å‰ä»»åŠ¡çŠ¶æ€
            task_status_log = {
                "timestamp": datetime.now().isoformat(),
                "type": "status",
                "message": f"å½“å‰ä»»åŠ¡çŠ¶æ€: {generation_tasks[task_id]['status']}",
                "task_status": generation_tasks[task_id]['status'],
                "progress": generation_tasks[task_id].get('progress', ''),
            }
            data = json.dumps(task_status_log, ensure_ascii=False)
            yield f"data: {data}\n\n"
            
            # å®æ—¶æ¨é€æ–°æ—¥å¿—
            while True:
                try:
                    # ç­‰å¾…æ–°çš„æ—¥å¿—æ¡ç›®ï¼Œè®¾ç½®è¶…æ—¶é˜²æ­¢è¿æ¥æŒ‚èµ·
                    log_entry = await asyncio.wait_for(log_queue.get(), timeout=30.0)
                    data = json.dumps(log_entry, ensure_ascii=False)
                    yield f"data: {data}\n\n"
                    
                    # å¦‚æœä»»åŠ¡å·²å®Œæˆæˆ–å¤±è´¥ï¼Œå‘é€ç»“æŸä¿¡å·
                    if log_entry.get('type') in ['success', 'error'] or log_entry.get('step') == 'ä»»åŠ¡å®Œæˆ':
                        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´å†ç»“æŸè¿æ¥
                        await asyncio.sleep(1)
                        end_log = {
                            "timestamp": datetime.now().isoformat(),
                            "type": "stream_end",
                            "message": "æ—¥å¿—æµç»“æŸ"
                        }
                        data = json.dumps(end_log, ensure_ascii=False)
                        yield f"data: {data}\n\n"
                        break
                        
                except asyncio.TimeoutError:
                    # å‘é€å¿ƒè·³ï¼Œä¿æŒè¿æ¥æ´»è·ƒ
                    heartbeat = {
                        "timestamp": datetime.now().isoformat(),
                        "type": "heartbeat",
                        "message": "è¿æ¥æ­£å¸¸"
                    }
                    data = json.dumps(heartbeat, ensure_ascii=False)
                    yield f"data: {data}\n\n"
                    
        except Exception as e:
            # å‘é€é”™è¯¯ä¿¡æ¯
            error_log = {
                "timestamp": datetime.now().isoformat(),
                "type": "stream_error",
                "message": f"æ—¥å¿—æµå¼‚å¸¸: {str(e)}"
            }
            data = json.dumps(error_log, ensure_ascii=False)
            yield f"data: {data}\n\n"
            
        finally:
            # æ¸…ç†è®¢é˜…
            if log_queue:
                log_manager.unsubscribe_logs(task_id, log_queue)
    
    return StreamingResponse(
        log_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Cache-Control",
        }
    )

@app.get("/logs/{task_id}")
async def get_task_logs(task_id: str):
    """è·å–ä»»åŠ¡çš„å†å²æ—¥å¿—"""
    
    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
    if task_id not in generation_tasks:
        raise HTTPException(status_code=404, detail=f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
    
    logs = log_manager.get_logs(task_id)
    
    return {
        "task_id": task_id,
        "task_status": generation_tasks[task_id]["status"],
        "log_count": len(logs),
        "logs": logs,
        "last_updated": generation_tasks[task_id]["updated_at"].isoformat()
    }

@app.get("/status", response_model=SystemStatus)
async def get_system_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    active_tasks = sum(1 for task in generation_tasks.values() 
                      if task["status"] in ["pending", "running"])
    
    # è®¡ç®—è¿è¡Œæ—¶é—´ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    uptime = "è¿è¡Œä¸­"
    
    # æ£€æŸ¥MinIOçŠ¶æ€
    minio_client = get_minio_client()
    minio_status = "available" if minio_client.is_available() else "unavailable"
    
    return SystemStatus(
        service="Gauzæ–‡æ¡£Agent API",
        status="running",
        version="1.0.0",
        active_tasks=active_tasks,
        total_tasks=len(generation_tasks),
        concurrency_settings=pipeline.get_concurrency_settings(),
        uptime=uptime,
        minio_status=minio_status
    )

@app.post("/set_concurrency")
async def set_concurrency(settings: ConcurrencySettings):
    """è®¾ç½®å¹¶å‘å‚æ•°"""
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    try:
        pipeline.set_concurrency(
            orchestrator_workers=settings.orchestrator_workers,
            react_workers=settings.react_workers,
            content_workers=settings.content_workers,
            rate_delay=settings.rate_delay
        )
        
        logger.info(f"âœ… å¹¶å‘è®¾ç½®å·²æ›´æ–°: {settings.dict()}")
        
        return {
            "status": "success",
            "message": "å¹¶å‘è®¾ç½®å·²æ›´æ–°",
            "current_settings": pipeline.get_concurrency_settings()
        }
        
    except Exception as e:
        logger.error(f"âŒ è®¾ç½®å¹¶å‘å‚æ•°å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è®¾ç½®å¤±è´¥: {str(e)}")

@app.post("/generate_document", response_model=DocumentGenerationResponse)
async def generate_document(request: DocumentGenerationRequest, background_tasks: BackgroundTasks):
    """
    ç”Ÿæˆæ–‡æ¡£æ¥å£ - å¼‚æ­¥å¤„ç†
    
    æäº¤æ–‡æ¡£ç”Ÿæˆä»»åŠ¡ï¼Œè¿”å›ä»»åŠ¡IDã€‚å¯é€šè¿‡ä»»åŠ¡IDæŸ¥è¯¢è¿›åº¦å’Œä¸‹è½½ç»“æœã€‚
    """
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    # ç”Ÿæˆä»»åŠ¡ID
    task_id = str(uuid.uuid4())
    
    # åˆ›å»ºä»»åŠ¡è®°å½•
    task_info = {
        "task_id": task_id,
        "status": "pending",
        "progress": "ä»»åŠ¡å·²æäº¤ï¼Œç­‰å¾…å¤„ç†",
        "created_at": datetime.now(),
        "updated_at": datetime.now(),
        "request": request.dict(),
        "result": None,
        "error": None
    }
    
    generation_tasks[task_id] = task_info
    
    # æ·»åŠ åå°ä»»åŠ¡
    background_tasks.add_task(run_document_generation, task_id, request)
    
    logger.info(f"ğŸ“ æ–°çš„æ–‡æ¡£ç”Ÿæˆä»»åŠ¡: {task_id} - {request.query}")
    
    return DocumentGenerationResponse(
        task_id=task_id,
        status="pending",
        message=f"æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å·²æäº¤ï¼Œä»»åŠ¡ID: {task_id}",
        files=None
    )

@app.get("/tasks/{task_id}", response_model=TaskStatus)
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if task_id not in generation_tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task_info = generation_tasks[task_id]
    
    return TaskStatus(
        task_id=task_info["task_id"],
        status=task_info["status"],
        progress=task_info["progress"],
        created_at=task_info["created_at"],
        updated_at=task_info["updated_at"],
        request=task_info.get("request"),
        result=task_info["result"],
        error=task_info["error"]
    )

@app.get("/tasks")
async def list_tasks(limit: int = 20, status_filter: Optional[str] = None):
    """è·å–ä»»åŠ¡åˆ—è¡¨"""
    tasks = list(generation_tasks.values())
    
    # çŠ¶æ€è¿‡æ»¤
    if status_filter:
        tasks = [task for task in tasks if task["status"] == status_filter]
    
    # æŒ‰æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
    tasks.sort(key=lambda x: x["created_at"], reverse=True)
    
    # é™åˆ¶æ•°é‡
    tasks = tasks[:limit]
    
    return {
        "total": len(generation_tasks),
        "filtered": len(tasks),
        "tasks": tasks
    }

@app.get("/download/{file_id}")
async def download_file(file_id: str):
    """ä¸‹è½½ç”Ÿæˆçš„æ–‡ä»¶"""
    if file_id not in file_storage:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    file_path = file_storage[file_id]
    
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="æ–‡ä»¶å·²è¢«åˆ é™¤")
    
    filename = os.path.basename(file_path)
    
    return FileResponse(
        path=file_path,
        filename=filename,
        media_type='application/octet-stream'
    )

# ===== åå°ä»»åŠ¡å‡½æ•° =====

async def run_document_generation(task_id: str, request: DocumentGenerationRequest):
    """åå°æ‰§è¡Œæ–‡æ¡£ç”Ÿæˆä»»åŠ¡"""
    task_info = generation_tasks[task_id]
    
    try:
        # æ¨é€å¼€å§‹æ—¥å¿—
        log_manager.add_log(task_id, {
            "type": "info",
            "message": "æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å·²å¯åŠ¨",
            "progress": 0,
            "step": "ä»»åŠ¡åˆå§‹åŒ–",
            "query": request.query,
            "project_name": request.project_name
        })
        
        # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
        task_info["status"] = "running"
        task_info["progress"] = "æ­£åœ¨ç”Ÿæˆæ–‡æ¡£ç»“æ„..."
        task_info["updated_at"] = datetime.now()
        
        # æ¨é€çŠ¶æ€æ›´æ–°
        log_manager.add_log(task_id, {
            "type": "progress",
            "message": "æ­£åœ¨åˆå§‹åŒ–æ–‡æ¡£ç”Ÿæˆæµæ°´çº¿...",
            "progress": 5,
            "step": "æµæ°´çº¿åˆå§‹åŒ–"
        })
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ–‡æ¡£ç”Ÿæˆä»»åŠ¡: {task_id}")
        
        # åˆ›å»ºä»»åŠ¡ä¸“ç”¨è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = f"api_outputs/{task_id}_{timestamp}"
        
        # æ¨é€ç›®å½•åˆ›å»ºæ—¥å¿—
        log_manager.add_log(task_id, {
            "type": "progress",
            "message": f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}",
            "progress": 10,
            "step": "ç›®å½•åˆ›å»º",
            "output_dir": output_dir
        })
        
        # æ¨é€æ–‡æ¡£ç”Ÿæˆå¼€å§‹
        log_manager.add_log(task_id, {
            "type": "progress",
            "message": "å¼€å§‹æ‰§è¡Œå¤šAgentæ–‡æ¡£ç”Ÿæˆæµæ°´çº¿...",
            "progress": 15,
            "step": "å¤šAgentåä½œ"
        })
        
        # åœ¨æ–°çš„çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥ä»£ç 
        loop = asyncio.get_event_loop()
        result_files = await loop.run_in_executor(
            None, 
            pipeline.generate_document, 
            request.query,
            request.project_name,
            output_dir
        )
        
        # æ¨é€æ–‡æ¡£ç”Ÿæˆå®Œæˆ
        log_manager.add_log(task_id, {
            "type": "progress",
            "message": "æ–‡æ¡£å†…å®¹ç”Ÿæˆå®Œæˆï¼Œæ­£åœ¨å¤„ç†æ–‡ä»¶...",
            "progress": 70,
            "step": "æ–‡ä»¶å¤„ç†",
            "generated_files": list(result_files.keys())
        })
        
        # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶ä¸‹è½½é“¾æ¥
        file_links = {}
        for file_type, file_path in result_files.items():
            if file_type != 'output_directory' and os.path.exists(file_path):
                file_id = str(uuid.uuid4())
                file_storage[file_id] = file_path
                file_links[file_type] = f"/download/{file_id}"
        
        # ä¸Šä¼ æ–‡ä»¶åˆ°MinIO
        task_info["progress"] = "æ­£åœ¨ä¸Šä¼ æ–‡ä»¶åˆ°MinIO..."
        task_info["updated_at"] = datetime.now()
        
        # æ¨é€MinIOä¸Šä¼ å¼€å§‹
        log_manager.add_log(task_id, {
            "type": "progress",
            "message": "å¼€å§‹ä¸Šä¼ æ–‡ä»¶åˆ°äº‘å­˜å‚¨(MinIO)...",
            "progress": 80,
            "step": "äº‘å­˜å‚¨ä¸Šä¼ "
        })
        
        minio_urls = {}
        try:
            logger.info(f"ğŸ“¤ å¼€å§‹ä¸Šä¼ æ–‡ä»¶åˆ°MinIO: {task_id}")
            minio_urls = upload_document_files(result_files, task_id)
            if minio_urls:
                logger.info(f"âœ… MinIOä¸Šä¼ æˆåŠŸ: {len(minio_urls)} ä¸ªæ–‡ä»¶")
                log_manager.add_log(task_id, {
                    "type": "success",
                    "message": f"äº‘å­˜å‚¨ä¸Šä¼ æˆåŠŸï¼Œå…±ä¸Šä¼  {len(minio_urls)} ä¸ªæ–‡ä»¶",
                    "progress": 90,
                    "step": "ä¸Šä¼ å®Œæˆ",
                    "minio_files": len(minio_urls)
                })
            else:
                logger.warning(f"âš ï¸ MinIOä¸Šä¼ å¤±è´¥ï¼Œä»…æä¾›æœ¬åœ°ä¸‹è½½")
                log_manager.add_log(task_id, {
                    "type": "warning",
                    "message": "äº‘å­˜å‚¨ä¸Šä¼ å¤±è´¥ï¼Œä»…æä¾›æœ¬åœ°ä¸‹è½½",
                    "progress": 85,
                    "step": "ä¸Šä¼ å¤±è´¥"
                })
        except Exception as e:
            logger.error(f"âŒ MinIOä¸Šä¼ å¼‚å¸¸: {e}")
            log_manager.add_log(task_id, {
                "type": "error",
                "message": f"äº‘å­˜å‚¨ä¸Šä¼ å¼‚å¸¸: {str(e)}",
                "progress": 85,
                "step": "ä¸Šä¼ å¼‚å¸¸",
                "error": str(e)
            })
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        task_info["status"] = "completed"
        task_info["progress"] = "æ–‡æ¡£ç”Ÿæˆå’Œä¸Šä¼ å®Œæˆ"
        task_info["result"] = {
            "files": file_links,
            "minio_urls": minio_urls,
            "output_directory": result_files.get("output_directory"),
            "generation_time": datetime.now().isoformat(),
            "storage_info": {
                "local_files": len(file_links),
                "minio_files": len(minio_urls),
                "total_size_mb": sum(
                    os.path.getsize(file_path) / (1024 * 1024) 
                    for file_path in result_files.values() 
                    if file_path != result_files.get("output_directory") and os.path.exists(file_path)
                )
            }
        }
        task_info["updated_at"] = datetime.now()
        
        # æ¨é€ä»»åŠ¡å®Œæˆæ—¥å¿—
        log_manager.add_log(task_id, {
            "type": "success",
            "message": "âœ… æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å®Œæˆï¼",
            "progress": 100,
            "step": "ä»»åŠ¡å®Œæˆ",
            "result": {
                "minio_urls": minio_urls,
                "local_files": file_links,
                "storage_info": task_info["result"]["storage_info"]
            }
        })
        
        logger.info(f"âœ… æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å®Œæˆ: {task_id}")
        
    except Exception as e:
        # æ¨é€é”™è¯¯æ—¥å¿—
        log_manager.add_log(task_id, {
            "type": "error",
            "message": f"âŒ æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å¤±è´¥: {str(e)}",
            "progress": 0,
            "step": "ä»»åŠ¡å¤±è´¥",
            "error": str(e)
        })
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        task_info["status"] = "failed"
        task_info["progress"] = f"ç”Ÿæˆå¤±è´¥: {str(e)}"
        task_info["error"] = str(e)
        task_info["updated_at"] = datetime.now()
        
        logger.error(f"âŒ æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å¤±è´¥: {task_id} - {e}")
    finally:
        # ä»»åŠ¡å®Œæˆåæ¸…ç†æ—¥å¿—è®¢é˜…è€…ï¼ˆä½†ä¿ç•™æ—¥å¿—1å°æ—¶ï¼‰
        log_manager.cleanup_task_logs(task_id)

# ===== å¯åŠ¨æœåŠ¡å™¨ =====

def start_server(host: str = "0.0.0.0", port: int = 8000, reload: bool = False):
    """å¯åŠ¨FastAPIæœåŠ¡å™¨"""
    print("ğŸš€ å¯åŠ¨Gauzæ–‡æ¡£Agent APIæœåŠ¡å™¨...")
    print(f"ğŸ“Š æœåŠ¡åœ°å€: http://{host}:{port}")
    print(f"ğŸ“– APIæ–‡æ¡£: http://{host}:{port}/docs")
    print(f"ğŸ“š ReDocæ–‡æ¡£: http://{host}:{port}/redoc")
    
    uvicorn.run(
        "api_server:app",
        host=host,
        port=port,
        reload=reload,
        log_level="info"
    )

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Gauzæ–‡æ¡£Agent APIæœåŠ¡å™¨")
    parser.add_argument("--host", default="0.0.0.0", help="æœåŠ¡å™¨ä¸»æœºåœ°å€")
    parser.add_argument("--port", type=int, default=8002, help="æœåŠ¡å™¨ç«¯å£")
    parser.add_argument("--reload", action="store_true", help="å¼€å‘æ¨¡å¼è‡ªåŠ¨é‡è½½")
    
    args = parser.parse_args()
    start_server(args.host, args.port, args.reload) 