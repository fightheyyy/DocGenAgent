"""
PDFè§£æå·¥å…· - åŸºäºPaper2Posterçš„æ™ºèƒ½PDFè§£æåŠŸèƒ½
æ”¯æŒå¤šç§AIæ¨¡å‹ï¼Œæ™ºèƒ½æå–PDFä¸­çš„æ–‡æœ¬ã€å›¾ç‰‡å’Œè¡¨æ ¼
åˆ©ç”¨Paper2Posteré¡¹ç›®ä¸­çš„ç°æœ‰èµ„æºå’Œåº“
"""

import os
import json
import sys
import random
import re
import shutil
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å®šä¹‰é»˜è®¤çš„è§£æè¾“å‡ºç›®å½•
DEFAULT_PARSER_OUTPUT_DIR = project_root / "parser_output"

# æ·»åŠ Paper2Posterè·¯å¾„
paper2poster_dir = project_root / "Paper2Poster" / "Paper2Poster"
if paper2poster_dir.exists():
    # æ·»åŠ Paper2Posterä¸»ç›®å½•
    p2p_path = str(paper2poster_dir)
    if p2p_path not in sys.path:
        sys.path.insert(0, p2p_path)
    print(f"âœ… æ·»åŠ Paper2Posterè·¯å¾„: {p2p_path}")
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç°æˆçš„è§£æå™¨
    openrouter_parser = paper2poster_dir / "parser_agent_openrouter.py"
    simple_parser = paper2poster_dir / "simple_docling_parser.py"
    
    if openrouter_parser.exists():
        print(f"âœ… å‘ç°OpenRouterè§£æå™¨: {openrouter_parser}")
    if simple_parser.exists():
        print(f"âœ… å‘ç°ç®€å•è§£æå™¨: {simple_parser}")
else:
    print("âŒ æœªæ‰¾åˆ°Paper2Posterç›®å½•")

# å°è¯•å¯¼å…¥Paper2Posterçš„ä¾èµ–
try:
    # é¦–å…ˆå°è¯•ä»Paper2Posterå¯¼å…¥ç°æœ‰çš„è§£æå™¨
    sys.path.insert(0, str(paper2poster_dir))
    
    # å¯¼å…¥åŸºç¡€ä¾èµ–
    from dotenv import load_dotenv
    from pathlib import Path
    import subprocess
    
    # å°è¯•å¯¼å…¥camelå’Œdoclingï¼ˆæ¥è‡ªPaper2Posterï¼‰
    from camel.models import ModelFactory
    from camel.agents import ChatAgent
    from camel.types import ModelPlatformType
    from docling_core.types.doc.document import PictureItem, TableItem
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.pipeline_options import PdfPipelineOptions, EasyOcrOptions
    from docling.document_converter import DocumentConverter, PdfFormatOption
    from PIL import Image
    
    DEPENDENCIES_AVAILABLE = True
    print("âœ… PDFè§£æä¾èµ–åº“æ£€æŸ¥é€šè¿‡ï¼ˆä½¿ç”¨Paper2Posteræœ¬åœ°åº“ï¼‰")
    
    # å°è¯•å¯¼å…¥Paper2Posterçš„ç°æœ‰è§£æå™¨
    try:
        import importlib.util
        
        # åŠ¨æ€å¯¼å…¥parser_agent_openrouter
        if (paper2poster_dir / "parser_agent_openrouter.py").exists():
            spec = importlib.util.spec_from_file_location(
                "parser_agent_openrouter", 
                paper2poster_dir / "parser_agent_openrouter.py"
            )
            parser_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(parser_module)
            
            # è·å–OpenRouterParserAgentç±»
            OpenRouterParserAgent = getattr(parser_module, 'OpenRouterParserAgent', None)
            if OpenRouterParserAgent:
                print("âœ… æˆåŠŸå¯¼å…¥Paper2Posterçš„OpenRouterParserAgent")
            else:
                print("âŒ æœªæ‰¾åˆ°OpenRouterParserAgentç±»")
        
        # åŠ¨æ€å¯¼å…¥simple_docling_parser
        if (paper2poster_dir / "simple_docling_parser.py").exists():
            spec_simple = importlib.util.spec_from_file_location(
                "simple_docling_parser", 
                paper2poster_dir / "simple_docling_parser.py"
            )
            simple_module = importlib.util.module_from_spec(spec_simple)
            spec_simple.loader.exec_module(simple_module)
            
            # è·å–ç®€å•è§£æå‡½æ•°
            parse_pdf_simple = getattr(simple_module, 'parse_pdf_simple', None)
            if parse_pdf_simple:
                print("âœ… æˆåŠŸå¯¼å…¥Paper2Posterçš„parse_pdf_simple")
        
        PAPER2POSTER_PARSERS_AVAILABLE = True
    except Exception as e:
        print(f"âš ï¸ Paper2Posterè§£æå™¨å¯¼å…¥å¤±è´¥: {e}")
        PAPER2POSTER_PARSERS_AVAILABLE = False
        OpenRouterParserAgent = None
        parse_pdf_simple = None
        
except ImportError as e:
    DEPENDENCIES_AVAILABLE = False
    PAPER2POSTER_PARSERS_AVAILABLE = False
    print(f"âŒ PDFè§£æä¾èµ–åº“ä¸å¯ç”¨: {e}")
    print("æç¤ºï¼šè¯·ç¡®è®¤Paper2Posterç›®å½•å­˜åœ¨ä¸”åŒ…å«camelå’Œdoclingåº“")
    OpenRouterParserAgent = None
    parse_pdf_simple = None

# å°è¯•å¯¼å…¥torchï¼ˆå¯é€‰ï¼‰
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("PyTorchä¸å¯ç”¨ï¼ŒæŸäº›é«˜çº§åŠŸèƒ½å¯èƒ½å—é™")

from src.base_tool import Tool
# --- Import the embedding service ---
try:
    from src.pdf_embedding_service import PDFEmbeddingService
    EMBEDDING_SERVICE_AVAILABLE = True
    print("âœ… PDF Embedding Serviceå¯ç”¨")
except ImportError as e:
    EMBEDDING_SERVICE_AVAILABLE = False
    print(f"âš ï¸ PDF Embedding Serviceä¸å¯ç”¨: {e}")

# --- Import the OpenRouter client for image description ---
try:
    from src.openrouter_client import OpenRouterClient
    OPENROUTER_CLIENT_AVAILABLE = True
    print("âœ… OpenRouter Clientå¯ç”¨")
except ImportError as e:
    OPENROUTER_CLIENT_AVAILABLE = False
    print(f"âš ï¸ OpenRouter Clientä¸å¯ç”¨: {e}")


# åŠ è½½ç¯å¢ƒå˜é‡
if DEPENDENCIES_AVAILABLE:
    load_dotenv()

# é…ç½®å¸¸é‡
IMAGE_RESOLUTION_SCALE = 5.0

# æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ï¼ˆä»Paper2Posterç»§æ‰¿ï¼‰
SUPPORTED_MODELS = [
    "gpt-4o", "gpt-4o-mini", "gpt-3.5-turbo",
    "claude-3.5-sonnet", "claude-3-haiku", 
    "llama-3.1-8b", "llama-3.1-70b",
    "gemini-pro", "qwen2.5-7b", "qwen2.5-32b"
]

class OpenRouterParserAgent:
    """æ”¯æŒOpenRouterçš„è§£ææ™ºèƒ½ä½“ç±»"""
    
    def __init__(self, model_name: str = "gpt-4o"):
        """
        åˆå§‹åŒ–è§£ææ™ºèƒ½ä½“
        
        Args:
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œæ”¯æŒOpenRouterä¸Šçš„å¤šç§æ¨¡å‹
        """
        self.model_name = model_name
        self.actor_model = None
        self.actor_agent = None
        self.doc_converter = None
        
        if DEPENDENCIES_AVAILABLE:
            self._init_components()
    
    def _init_components(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        try:
            # åˆå§‹åŒ–Doclingè½¬æ¢å™¨
            self._init_docling_converter()
            
            # åˆå§‹åŒ–AIæ¨¡å‹
            self._init_ai_model()
            
            print(f"âœ… OpenRouterè§£ææ™ºèƒ½ä½“åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {self.model_name}")
        except Exception as e:
            print(f"âŒ OpenRouterè§£ææ™ºèƒ½ä½“åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _init_docling_converter(self):
        """åˆå§‹åŒ–Doclingè½¬æ¢å™¨"""
        try:
            # è®¾ç½®æœ¬åœ°æ¨¡å‹è·¯å¾„
            models_cache_dir = Path("models_cache")
            if models_cache_dir.exists():
                artifacts_path = str(models_cache_dir.absolute())
            else:
                artifacts_path = None

            pipeline_options = PdfPipelineOptions(
                ocr_options=EasyOcrOptions(),
                artifacts_path=artifacts_path
            )
            pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE
            pipeline_options.generate_page_images = True
            pipeline_options.generate_picture_images = True

            self.doc_converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
                }
            )
            print("âœ… Doclingè½¬æ¢å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Doclingè½¬æ¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.doc_converter = None
    
    def _init_ai_model(self):
        """åˆå§‹åŒ–AIæ¨¡å‹"""
        try:
            # æ£€æŸ¥APIå¯†é’¥
            openrouter_api_key = os.getenv('OPENROUTER_API_KEY') or os.getenv('OPENAI_API_KEY')
            if not openrouter_api_key:
                print("âš ï¸ æœªè®¾ç½®OPENROUTER_API_KEYæˆ–OPENAI_API_KEYï¼ŒAIåŠŸèƒ½å°†ä¸å¯ç”¨")
                return
            
            # åˆ›å»ºæ¨¡å‹
            from camel.configs.openai_config import ChatGPTConfig
            
            # ä¸´æ—¶è®¾ç½®ç¯å¢ƒå˜é‡è¿æ¥åˆ°OpenRouter
            original_base_url = os.environ.get('OPENAI_API_BASE_URL')
            original_api_key = os.environ.get('OPENAI_API_KEY')
            
            os.environ['OPENAI_API_BASE_URL'] = 'https://openrouter.ai/api/v1'
            os.environ['OPENAI_API_KEY'] = openrouter_api_key
            
            try:
                self.actor_model = ModelFactory.create(
                    model_platform=ModelPlatformType.OPENAI,
                    model_type=self.model_name,
                    model_config_dict=ChatGPTConfig().as_dict(),
                )
                
                # åˆ›å»ºèŠå¤©æ™ºèƒ½ä½“
                actor_sys_msg = 'You are a document content divider and extractor specialist, expert in reorganizing document content into structured format.'
                
                self.actor_agent = ChatAgent(
                    system_message=actor_sys_msg,
                    model=self.actor_model,
                    message_window_size=10,
                    token_limit=None
                )
                
                print(f"âœ… AIæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {self.model_name}")
                
            finally:
                # æ¢å¤åŸå§‹ç¯å¢ƒå˜é‡
                if original_base_url:
                    os.environ['OPENAI_API_BASE_URL'] = original_base_url
                elif 'OPENAI_API_BASE_URL' in os.environ:
                    del os.environ['OPENAI_API_BASE_URL']
                
                if original_api_key:
                    os.environ['OPENAI_API_KEY'] = original_api_key
                elif original_api_key is None and 'OPENAI_API_KEY' in os.environ:
                    del os.environ['OPENAI_API_KEY']
            
        except Exception as e:
            print(f"âŒ AIæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.actor_model = None
            self.actor_agent = None
    
    def parse_raw(self, pdf_path: str, output_dir: str = None) -> Tuple[Dict, Dict, Dict]:
        """
        è§£æPDFæ–‡ä»¶
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„parser_output
            
        Returns:
            Tuple[Dict, Dict, Dict]: (content_json, images, tables)
        """
        if output_dir is None:
            output_dir = str(DEFAULT_PARSER_OUTPUT_DIR)

        print(f"ğŸ”„ å¼€å§‹è§£æPDF: {pdf_path}")
        print(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {self.model_name}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        if not self.doc_converter:
            raise Exception("Doclingè½¬æ¢å™¨æœªåˆå§‹åŒ–")
        
        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨Doclingè§£æPDF
        print("ğŸ“„ ä½¿ç”¨Doclingè§£æPDF...")
        raw_result = self.doc_converter.convert(pdf_path)
        raw_markdown = raw_result.document.export_to_markdown()
        
        # æ¸…ç†markdownå†…å®¹
        markdown_clean_pattern = re.compile(r"<!--[\s\S]*?-->")
        text_content = markdown_clean_pattern.sub("", raw_markdown)
        
        print(f"ğŸ“ è§£æå¾—åˆ°æ–‡æœ¬é•¿åº¦: {len(text_content)} å­—ç¬¦")
        
        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨LLMé‡æ–°ç»„ç»‡å†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        content_json = {}
        if self.actor_agent and len(text_content.strip()) > 0:
            print("ğŸ§  ä½¿ç”¨LLMé‡æ–°ç»„ç»‡å†…å®¹...")
            try:
                content_json = self._reorganize_content_with_llm(text_content)
            except Exception as e:
                print(f"âš ï¸ LLMé‡ç»„å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€ç»“æ„: {e}")
                content_json = self._create_basic_structure(text_content)
        else:
            print("âš ï¸ AIæ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€ç»“æ„åŒ–å¤„ç†")
            content_json = self._create_basic_structure(text_content)
        
        # ç¬¬ä¸‰æ­¥ï¼šæå–å›¾ç‰‡å’Œè¡¨æ ¼
        print("ğŸ–¼ï¸ æå–å›¾ç‰‡å’Œè¡¨æ ¼...")
        images, tables = self._extract_images_and_tables(raw_result, output_dir)
        
        # ä¿å­˜ç»“æœ
        self._save_results(content_json, images, tables, output_dir)
        
        return content_json, images, tables
    
    def _create_basic_structure(self, text_content: str) -> Dict:
        """åˆ›å»ºåŸºç¡€æ–‡æ¡£ç»“æ„ï¼ˆå½“AIä¸å¯ç”¨æ—¶ï¼‰- ä¿ç•™æ›´å¤šåŸå§‹æ–‡æœ¬"""
        # æŒ‰è¡Œåˆ†å‰²ï¼Œä¿ç•™æ‰€æœ‰éç©ºè¡Œ
        lines = [line.strip() for line in text_content.split('\n') if line.strip()]
        
        # åŸºç¡€ç»“æ„
        content_json = {
            "sections": []
        }
        
        # å°†æ–‡æœ¬æŒ‰é€»è¾‘åˆ†ç»„ï¼Œæ¯ä¸ªsectionåŒ…å«æ›´å¤šå†…å®¹
        current_section = []
        current_page = 0
        
        for line in lines:
            current_section.append(line)
            
            # æ¯çº¦3000å­—ç¬¦æˆ–é‡åˆ°æ˜æ˜¾çš„åˆ†é¡µæ ‡å¿—æ—¶åˆ›å»ºæ–°section
            if (len('\n'.join(current_section)) > 3000 or 
                any(keyword in line.lower() for keyword in ['page ', 'ç¬¬', 'é¡µ', '---', '==='])):
                
                if current_section:
                    section_content = '\n'.join(current_section)
                    content_json["sections"].append({
                        "content": section_content,
                        "source_page": current_page
                    })
                    current_section = []
                    current_page += 1
        
        # æ·»åŠ æœ€åä¸€ä¸ªsection
        if current_section:
            section_content = '\n'.join(current_section)
            content_json["sections"].append({
                "content": section_content,
                "source_page": current_page
            })
        
        return content_json
    
    def _reorganize_content_with_llm(self, text_content: str) -> Dict:
        """ä½¿ç”¨LLMé‡æ–°ç»„ç»‡å†…å®¹"""
        # æç¤ºæ¨¡æ¿
        template_content = """You are a document content divider and extractor specialist, expert in dividing and extracting content from various types of documents and reorganizing it into a two-level json format.

Based on given markdown document, generate a JSON output, make sure the output is concise and focused.

Step-by-Step Instructions:
1. Identify Sections and Subsections in document and identify sections and subsections based on the heading levels and logical structure.

2. Divide Content: Reorganize the content into sections and subsections, ensuring that each subsection contains approximately 500 words.

3. Refine Titles: Create titles for each section with at most 3 words.

4. Remove Unwanted Elements: Eliminate any unwanted elements such as headers, footers, text surrounded by `~~` indicating deletion.

5. Refine Text: For content, you should keep as much raw text as possible. Do not include citations.

6. Length: you should control the length of each section, according to their importance according to your understanding of the document. For important sections, their content should be long.

7. Make sure there is a document title section at the beginning, and it should contain information like document title, author, organization etc.

8. The "meta" key contains the meta information of the document, where the title should be the raw title of the document and is not summarized.

9. There **must** be a section for the document title.

Example Output:
{
    "meta": {
        "poster_title": "raw title of the document",
        "authors": "authors of the document",
        "affiliations": "affiliations of the authors"
    },
    "sections": [
        {
            "title": "Document Title",
            "content": "content of document title and author"
        },
        {
            "title": "Introduction",
            "content": "content of introduction section"
        },
        {
            "title": "Methods",
            "content": "content of methods section"
        }
    ]
}

Give your output in JSON format
Input:
{{ markdown_document }}
Output:"""
        
        from jinja2 import Template
        template = Template(template_content)
        prompt = template.render(markdown_document=text_content[:50000])  # é™åˆ¶é•¿åº¦é˜²æ­¢è¶…é™
        
        # è°ƒç”¨LLM
        max_retries = 3
        for attempt in range(max_retries):
            try:
                self.actor_agent.reset()
                response = self.actor_agent.step(prompt)
                
                # æå–JSON
                content_json = self._get_json_from_response(response.msgs[0].content)
                
                if len(content_json.get("sections", [])) > 0:
                    # éªŒè¯å’Œä¼˜åŒ–ç»“æœ
                    content_json = self._validate_and_optimize_content(content_json)
                    return content_json
                else:
                    print(f"âš ï¸ LLMè¿”å›ç©ºç»“æœï¼Œé‡è¯•... ({attempt + 1}/{max_retries})")
                    
            except Exception as e:
                print(f"âš ï¸ LLMè°ƒç”¨å¤±è´¥ ({attempt + 1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    raise e
        
        raise Exception("LLMå¤„ç†å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
    
    def _get_json_from_response(self, content: str) -> Dict:
        """ä»å“åº”ä¸­æå–JSON"""
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            return json.loads(content)
        except:
            # å°è¯•æå–JSONä»£ç å—
            json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))
            else:
                # å°è¯•æå–{}åŒ…å›´çš„å†…å®¹
                brace_match = re.search(r'\{.*\}', content, re.DOTALL)
                if brace_match:
                    return json.loads(brace_match.group(0))
                else:
                    raise ValueError("Could not extract JSON from response")
    
    def _validate_and_optimize_content(self, content_json: Dict) -> Dict:
        """éªŒè¯å’Œä¼˜åŒ–å†…å®¹"""
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        if 'sections' not in content_json:
            raise ValueError("Missing 'sections' field in response")
        
        # éªŒè¯æ¯ä¸ªsectionçš„æ ¼å¼
        valid_sections = []
        for section in content_json['sections']:
            if isinstance(section, dict) and 'title' in section and 'content' in section:
                valid_sections.append(section)
        
        content_json['sections'] = valid_sections
        
        if len(valid_sections) == 0:
            raise ValueError("No valid sections found")
        
        # å¦‚æœsectionè¿‡å¤šï¼Œè¿›è¡Œæ™ºèƒ½é€‰æ‹©
        if len(content_json['sections']) > 9:
            print(f"âš ï¸ Sectionæ•°é‡è¿‡å¤š({len(content_json['sections'])}ä¸ª)ï¼Œè¿›è¡Œæ™ºèƒ½é€‰æ‹©...")
            selected_sections = (
                content_json['sections'][:2] + 
                random.sample(content_json['sections'][2:-2], min(5, len(content_json['sections'][2:-2]))) + 
                content_json['sections'][-2:]
            )
            content_json['sections'] = selected_sections
            print(f"âœ… é€‰æ‹©åå‰©ä½™{len(content_json['sections'])}ä¸ªsections")
        
        return content_json
    
    def _extract_images_and_tables(self, raw_result, output_dir: str) -> Tuple[Dict, Dict]:
        """æå–å›¾ç‰‡å’Œè¡¨æ ¼"""
        images = {}
        tables = {}
        
        # æå–è¡¨æ ¼
        table_counter = 0
        for element, _level in raw_result.document.iterate_items():
            if isinstance(element, TableItem):
                table_counter += 1
                caption = element.caption_text(raw_result.document)
                table_img_path = os.path.join(output_dir, f"table-{table_counter}.png")
                
                # ä¿å­˜è¡¨æ ¼å›¾ç‰‡
                try:
                    table_image = element.get_image(raw_result.document)
                    if table_image is not None:
                        with open(table_img_path, "wb") as fp:
                            table_image.save(fp, "PNG")
                        
                        # è·å–å›¾ç‰‡ä¿¡æ¯
                        table_img = Image.open(table_img_path)
                        tables[str(table_counter)] = {
                            'caption': caption if caption else f"è¡¨æ ¼ {table_counter}",
                            'table_path': table_img_path,
                            'width': table_img.width,
                            'height': table_img.height,
                            'figure_size': table_img.width * table_img.height,
                            'figure_aspect': table_img.width / table_img.height,
                        }
                        print(f"âœ… ä¿å­˜è¡¨æ ¼ {table_counter}: {table_img_path}")
                    else:
                        print(f"âš ï¸ è¡¨æ ¼ {table_counter} å›¾åƒä¸ºç©º")
                except Exception as e:
                    print(f"âŒ ä¿å­˜è¡¨æ ¼ {table_counter} å¤±è´¥: {e}")
        
        # æå–å›¾ç‰‡
        picture_counter = 0
        # æ”¶é›†æ‰€æœ‰å…ƒç´ ï¼Œä¾¿äºå®šä½å›¾ç‰‡å’Œæ–‡æœ¬
        all_elements = list(raw_result.document.iterate_items())
        for idx, (element, _level) in enumerate(all_elements):
            if isinstance(element, PictureItem):
                picture_counter += 1
                caption = element.caption_text(raw_result.document)
                image_img_path = os.path.join(output_dir, f"picture-{picture_counter}.png")
                try:
                    picture_image = element.get_image(raw_result.document)
                    if picture_image is not None:
                        with open(image_img_path, "wb") as fp:
                            picture_image.save(fp, "PNG")
                        image_img = Image.open(image_img_path)
                        
                        # [å·²ç§»é™¤] ä¸å†æå–contextå­—æ®µï¼Œä½¿ç”¨VLMè¿›è¡Œå›¾ç‰‡æè¿°
                        
                        images[str(picture_counter)] = {
                            'caption': caption if caption else f"å›¾ç‰‡ {picture_counter}",
                            'image_path': image_img_path,
                            'width': image_img.width,
                            'height': image_img.height,
                            'figure_size': image_img.width * image_img.height,
                            'figure_aspect': image_img.width / image_img.height,
                            # [å·²ç§»é™¤] 'context': context, - ä¸å†ä½¿ç”¨contextå­—æ®µï¼Œç”±VLMç”Ÿæˆæè¿°
                        }
                        print(f"âœ… ä¿å­˜å›¾ç‰‡ {picture_counter}: {image_img_path}")
                    else:
                        print(f"âš ï¸ å›¾ç‰‡ {picture_counter} å›¾åƒä¸ºç©º")
                except Exception as e:
                    print(f"âŒ ä¿å­˜å›¾ç‰‡ {picture_counter} å¤±è´¥: {e}")
        
        print(f"ğŸ“Š æå–äº† {len(tables)} ä¸ªè¡¨æ ¼å’Œ {len(images)} ä¸ªå›¾ç‰‡")
        return images, tables
    
    def _save_results(self, content_json: Dict, images: Dict, tables: Dict, output_dir: str):
        """ä¿å­˜è§£æç»“æœ"""
        # ä¿å­˜ç»“æ„åŒ–å†…å®¹
        content_path = os.path.join(output_dir, "parsed_content.json")
        with open(content_path, 'w', encoding='utf-8') as f:
            json.dump(content_json, f, indent=4, ensure_ascii=False)
        print(f"ğŸ“„ ç»“æ„åŒ–å†…å®¹å·²ä¿å­˜åˆ°: {content_path}")
        
        # ä¿å­˜å›¾ç‰‡ä¿¡æ¯
        images_path = os.path.join(output_dir, "images.json")
        with open(images_path, 'w', encoding='utf-8') as f:
            json.dump(images, f, indent=4, ensure_ascii=False)
        print(f"ğŸ–¼ï¸ å›¾ç‰‡ä¿¡æ¯å·²ä¿å­˜åˆ°: {images_path}")
        
        # ä¿å­˜è¡¨æ ¼ä¿¡æ¯
        tables_path = os.path.join(output_dir, "tables.json")
        with open(tables_path, 'w', encoding='utf-8') as f:
            json.dump(tables, f, indent=4, ensure_ascii=False)
        print(f"ğŸ“Š è¡¨æ ¼ä¿¡æ¯å·²ä¿å­˜åˆ°: {tables_path}")
        
        # ä¿å­˜æ±‡æ€»ä¿¡æ¯
        summary = {
            "total_sections": len(content_json.get("sections", [])),
            "total_images": len(images),
            "total_tables": len(tables),
            "meta_info": content_json.get("meta", {}),
            "section_titles": [section.get("title", "") for section in content_json.get("sections", [])],
            "model_used": self.model_name
        }
        
        summary_path = os.path.join(output_dir, "summary.json")
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=4, ensure_ascii=False)
        print(f"ğŸ“‹ æ±‡æ€»ä¿¡æ¯å·²ä¿å­˜åˆ°: {summary_path}")
    
    def get_parsing_stats(self, content_json: Dict, images: Dict, tables: Dict) -> Dict:
        """è·å–è§£æç»“æœçš„ç»Ÿè®¡ä¿¡æ¯"""
        # è®¡ç®—æ€»æ–‡æœ¬é•¿åº¦
        total_text_length = 0
        sections = content_json.get('sections', [])
        for section in sections:
            total_text_length += len(section.get('content', ''))
        
        stats = {
            'title': content_json.get('metadata', {}).get('title', content_json.get('title', 'N/A')),
            'sections_count': len(sections),
            'images_count': len(images),
            'tables_count': len(tables),
            'total_text_length': total_text_length,
            'model_used': getattr(self, 'model_name', 'unknown')
        }
        return stats

    def parse_without_llm(self, pdf_path: str, output_dir: str = None) -> tuple[dict, dict, dict]:
        """
        ç›´æ¥ä½¿ç”¨Doclingè¿›è¡Œè§£æï¼Œä¸é€šè¿‡LLMé‡ç»„å†…å®¹ã€‚
        è¿™å¯¹äºç»“æ„åŒ–è¾ƒå¥½çš„æ–‡æ¡£æˆ–ä¸éœ€è¦æ™ºèƒ½é‡ç»„çš„åœºæ™¯æ›´é«˜æ•ˆã€‚
        """
        if output_dir is None:
            output_dir = str(DEFAULT_PARSER_OUTPUT_DIR)

        print(f"ğŸ“„ è·³è¿‡LLMé‡ç»„ï¼Œæ‰§è¡ŒåŸå§‹è§£æ...")
        if not self.doc_converter:
            raise RuntimeError("Doclingè½¬æ¢å™¨æœªåˆå§‹åŒ–ã€‚")

        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        print(f"ğŸ“„ ä½¿ç”¨Doclingè¿›è¡ŒåŸå§‹è§£æ...")
        # ä½¿ç”¨doclingè½¬æ¢å™¨å¤„ç†PDF - ä¿®å¤å‚æ•°æ ¼å¼
        raw_result = self.doc_converter.convert(source=Path(pdf_path))
        print("âœ… DoclingåŸå§‹è§£æå®Œæˆã€‚")

        # ä½¿ç”¨ä¸parse_rawç›¸åŒçš„é€»è¾‘ï¼šå¯¼å‡ºä¸ºmarkdownç„¶åå¤„ç†
        raw_markdown = raw_result.document.export_to_markdown()
        
        # æ¸…ç†markdownå†…å®¹
        import re
        markdown_clean_pattern = re.compile(r"<!--[\s\S]*?-->")
        text_content = markdown_clean_pattern.sub("", raw_markdown)
        
        print(f"ğŸ“ è§£æå¾—åˆ°æ–‡æœ¬é•¿åº¦: {len(text_content)} å­—ç¬¦")

        # ä½¿ç”¨åŸºç¡€ç»“æ„åŒ–å¤„ç†ï¼ˆä¸ä½¿ç”¨LLMï¼‰
        content_json = self._create_basic_structure(text_content)
        
        # æ·»åŠ æºæ–‡ä»¶ä¿¡æ¯åˆ°metadata
        if "meta" in content_json:
            content_json["meta"]["source_file"] = pdf_path
        else:
            content_json["metadata"] = {
                "title": Path(pdf_path).stem,
                "source_file": pdf_path
            }
        
        # æå–å›¾ç‰‡å’Œè¡¨æ ¼
        images, tables = self._extract_images_and_tables(raw_result, output_dir)
        
        # ä¿å­˜ç»“æœ
        self._save_results(content_json, images, tables, output_dir)

        print("âœ… åŸå§‹è§£æå®Œæˆï¼Œå·²è·³è¿‡LLMé‡ç»„æ­¥éª¤ã€‚")
        return content_json, images, tables


class PDFParserTool(Tool):
    """PDFè§£æå·¥å…· - åŸºäºOpenRouterçš„æ™ºèƒ½PDFè§£æ"""
    
    def __init__(self):
        super().__init__(
            name="pdf_parser",
            description="ğŸ“„ PDFæ™ºèƒ½è§£æå·¥å…· - æå–æ–‡æœ¬ã€å›¾ç‰‡ã€è¡¨æ ¼å¹¶ç»“æ„åŒ–é‡ç»„ã€‚æ”¯æŒå¤šç§AIæ¨¡å‹ï¼Œæ™ºèƒ½å†…å®¹é‡ç»„ã€‚"
        )
        self.parser_agent = None
        self._init_parser()
    
    def _init_parser(self):
        """åˆå§‹åŒ–PDFè§£æå™¨"""
        if not DEPENDENCIES_AVAILABLE:
            print("âŒ PDFè§£æä¾èµ–åº“ä¸å¯ç”¨ï¼Œå·¥å…·åŠŸèƒ½å—é™")
            return
        
        try:
            # åˆ›å»ºé»˜è®¤è§£æå™¨
            self.parser_agent = OpenRouterParserAgent(model_name="gpt-4o")
            print("âœ… PDFè§£æå·¥å…·åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ PDFè§£æå·¥å…·åˆå§‹åŒ–å¤±è´¥: {e}")
            self.parser_agent = None
    
    def execute(self, **kwargs) -> str:
        """
        æ‰§è¡ŒPDFè§£æ
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º 'parser_output/YYYYMMDD_HHMMSS_random'ï¼‰
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºgpt-4oï¼‰
            action: æ“ä½œç±»å‹ï¼ˆparse/list_modelsï¼‰
            
        Returns:
            str: åŒ…å«è§£æç»“æœï¼ˆå¦‚è¾“å‡ºç›®å½•ï¼‰çš„JSONå­—ç¬¦ä¸²
        """
        action = kwargs.get("action", "parse")
        
        if action == "list_models":
            return self._list_available_models()
        elif action == "parse":
            return self._parse_pdf(**kwargs)
        else:
            return json.dumps({
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„æ“ä½œ: {action}ã€‚æ”¯æŒçš„æ“ä½œ: parse, list_models"
            }, indent=2, ensure_ascii=False)
    
    def _list_available_models(self) -> str:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„AIæ¨¡å‹"""
        return json.dumps({"supported_models": SUPPORTED_MODELS}, indent=2)
    
    def _parse_pdf(self, **kwargs) -> str:
        """è§£æPDFæ–‡ä»¶å¹¶è¿”å›ç»“æ„åŒ–ç»“æœ"""
        pdf_path = kwargs.get("pdf_path")
        
        if not pdf_path:
            return json.dumps({"status": "error", "message": "è¯·æä¾›PDFæ–‡ä»¶è·¯å¾„ (pdf_pathå‚æ•°)"}, indent=2)
        
        if not os.path.exists(pdf_path):
            return json.dumps({"status": "error", "message": f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}"}, indent=2)

        if not DEPENDENCIES_AVAILABLE:
            return json.dumps({"status": "error", "message": "PDFè§£æä¾èµ–åº“ä¸å¯ç”¨ï¼Œè¯·å®‰è£…å¿…è¦çš„ä¾èµ–åº“"}, indent=2)

        # ä¸ºæ¯ä¸ªè§£æä»»åŠ¡åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        random_id = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=6))
        
        # --- FIX: Use the absolute default path as the base ---
        base_output_dir = kwargs.get("output_dir", DEFAULT_PARSER_OUTPUT_DIR)
        output_dir = os.path.join(base_output_dir, f"{timestamp}_{random_id}")
        
        # ç¡®ä¿åŸºç¡€ç›®å½•å­˜åœ¨
        os.makedirs(base_output_dir, exist_ok=True)

        model_name = kwargs.get("model_name", "gpt-4o")
        
        try:
            # å¦‚æœæŒ‡å®šäº†ä¸åŒçš„æ¨¡å‹ï¼Œé‡æ–°åˆ›å»ºè§£æå™¨
            if (not self.parser_agent or 
                (hasattr(self.parser_agent, 'model_name') and 
                 self.parser_agent.model_name != model_name)):
                print(f"ğŸ”„ åˆ‡æ¢åˆ°æ¨¡å‹: {model_name}")
                self.parser_agent = OpenRouterParserAgent(model_name=model_name)
            
            if not self.parser_agent:
                raise Exception("PDFè§£æå™¨æœªæ­£ç¡®åˆå§‹åŒ–")
            
            print(f"ğŸš€ å¼€å§‹è§£æPDF: {pdf_path}")
            
            # æ–°å¢å‚æ•°æ§åˆ¶æ˜¯å¦ä½¿ç”¨LLM
            use_llm = kwargs.get('use_llm_reorganization', False)
            
            if use_llm:
                print("ğŸ§  ä½¿ç”¨LLMè¿›è¡Œå†…å®¹é‡ç»„...")
                content_json, images, tables = self.parser_agent.parse_raw(pdf_path, output_dir)
            else:
                print("ğŸ“„ è·³è¿‡LLMé‡ç»„ï¼Œæ‰§è¡ŒåŸå§‹è§£æ...")
                content_json, images, tables = self.parser_agent.parse_without_llm(pdf_path, output_dir)

            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = self.parser_agent.get_parsing_stats(content_json, images, tables)
            
            # --- ç»Ÿä¸€çš„PDFå†…å®¹embeddingå¤„ç† ---
            embedding_stats = {}
            if EMBEDDING_SERVICE_AVAILABLE:
                try:
                    print("ğŸ’¡ å¼€å§‹ç»Ÿä¸€çš„PDFå†…å®¹embeddingå¤„ç†...")
                    embedding_service = PDFEmbeddingService(enable_vlm_description=True)
                    
                    # ä½¿ç”¨ç»Ÿä¸€çš„embeddingæœåŠ¡å¤„ç†parsed_content.jsonå’Œimages.json
                    content_file_path = os.path.join(output_dir, "parsed_content.json")
                    images_file_path = os.path.join(output_dir, "images.json")
                    
                    # è°ƒç”¨ç»Ÿä¸€çš„embeddingæ–¹æ³•
                    embedding_result = embedding_service.embed_parsed_pdf(
                        parsed_content_path=content_file_path,
                        images_json_path=images_file_path,
                        parser_output_dir=output_dir
                    )
                    
                    # ç»Ÿä¸€çš„ç»“æœå¤„ç†
                    if not embedding_result.get("errors"):
                        embedding_stats = {
                            "status": "success",
                            "text_embeddings": embedding_result.get("text_embeddings", 0),
                            "image_embeddings": embedding_result.get("image_embeddings", 0),
                            "total_embeddings": embedding_result.get("total_embeddings", 0),
                            "method": "unified_embedding_service"
                        }
                        print(f"âœ… ç»Ÿä¸€embeddingå®Œæˆ: æ–‡æœ¬{embedding_stats['text_embeddings']}é¡¹, å›¾ç‰‡{embedding_stats['image_embeddings']}é¡¹")
                    else:
                        embedding_stats = {
                            "status": "partial_success",
                            "text_embeddings": embedding_result.get("text_embeddings", 0),
                            "image_embeddings": embedding_result.get("image_embeddings", 0),
                            "total_embeddings": embedding_result.get("total_embeddings", 0),
                            "errors": embedding_result.get("errors", []),
                            "method": "unified_embedding_service"
                        }
                        print(f"âš ï¸ ç»Ÿä¸€embeddingéƒ¨åˆ†æˆåŠŸ: æ–‡æœ¬{embedding_stats['text_embeddings']}é¡¹, å›¾ç‰‡{embedding_stats['image_embeddings']}é¡¹")
                        for error in embedding_result.get("errors", []):
                            print(f"  - é”™è¯¯: {error}")
                    
                except Exception as e:
                    print(f"âŒ ç»Ÿä¸€embeddingå¤„ç†å¤±è´¥: {e}")
                    embedding_stats = {"status": "error", "message": str(e)}
            else:
                embedding_stats = {"status": "skipped", "message": "Embedding service not available."}


            # å‡†å¤‡ç»“æ„åŒ–è¾“å‡º
            result = {
                "status": "success",
                "message": "PDFè§£æå®Œæˆ",
                "output_directory": output_dir,
                "embedding_info": embedding_stats,
                "statistics": {
                    "model_used": stats.get('model_used', 'unknown'),
                    "sections_count": stats.get('sections_count', 0),
                    "images_count": stats.get('images_count', 0),
                    "tables_count": stats.get('tables_count', 0),
                    "total_text_length": stats.get('total_text_length', 0)
                },
                "content_file": os.path.join(output_dir, "parsed_content.json"),
                "images_file": os.path.join(output_dir, "images.json") if images else None,
                "tables_file": os.path.join(output_dir, "tables.json") if tables else None
            }
            
            # è¿”å›JSONå­—ç¬¦ä¸²
            return json.dumps(result, indent=2, ensure_ascii=False)
            
        except Exception as e:
            # é”™è¯¯æ—¶ä¹Ÿè¿”å›JSON
            error_result = {
                "status": "error",
                "message": f"PDFè§£æå¤±è´¥: {str(e)}",
                "output_directory": None
            }
            return json.dumps(error_result, indent=2, ensure_ascii=False)
    
    def get_usage_guide(self) -> str:
        """è·å–ä½¿ç”¨æŒ‡å—"""
        return """
ğŸ“„ PDFè§£æå·¥å…·ä½¿ç”¨æŒ‡å—

ğŸ”§ åŸºæœ¬ç”¨æ³•:
1. è§£æPDF: pdf_parser(pdf_path="path/to/file.pdf", action="parse")
2. åˆ—å‡ºæ¨¡å‹: pdf_parser(action="list_models")
3. è·å–ç»Ÿè®¡: pdf_parser(action="get_stats", output_dir="parser_output")

ğŸ“‹ å‚æ•°è¯´æ˜:
- pdf_path: PDFæ–‡ä»¶è·¯å¾„ï¼ˆå¿…å¡«ï¼Œç”¨äºparseæ“ä½œï¼‰
- action: æ“ä½œç±»å‹ï¼ˆparse/list_models/get_statsï¼‰
- output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºparser_outputï¼‰
- model_name: AIæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºgpt-4oï¼‰
- use_llm_reorganization: æ˜¯å¦ä½¿ç”¨LLMè¿›è¡Œå†…å®¹é‡ç»„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºFalseï¼Œä»¥æé«˜é€Ÿåº¦ï¼‰

ğŸ§  æ”¯æŒçš„AIæ¨¡å‹:
- OpenAI: gpt-4o, gpt-4o-mini, gpt-3.5-turbo
- Anthropic: claude-3.5-sonnet, claude-3-haiku
- Meta: llama-3.1-8b, llama-3.1-70b
- Google: gemini-pro
- å¼€æº: qwen2.5-7b, qwen2.5-32b

ğŸ“ è¾“å‡ºæ–‡ä»¶:
- parsed_content.json: ç»“æ„åŒ–æ–‡æœ¬å†…å®¹
- images.json: å›¾ç‰‡ä¿¡æ¯
- tables.json: è¡¨æ ¼ä¿¡æ¯
- summary.json: æ±‡æ€»ä¿¡æ¯
- picture-*.png: æå–çš„å›¾ç‰‡æ–‡ä»¶
- table-*.png: æå–çš„è¡¨æ ¼æ–‡ä»¶

âš™ï¸ ç¯å¢ƒè¦æ±‚:
- è®¾ç½® OPENROUTER_API_KEY æˆ– OPENAI_API_KEY ç¯å¢ƒå˜é‡
- å®‰è£…å¿…è¦ä¾èµ–: camel-ai, docling, jinja2, pillow
""" 