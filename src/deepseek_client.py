"""
DeepSeek API Client for ReAct Agent
"""
import os
from typing import List, Dict, Any, Optional, Tuple
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class DeepSeekClient:
    """DeepSeek APIå®¢æˆ·ç«¯ï¼Œç”¨äºä¸DeepSeekæ¨¡å‹äº¤äº’"""
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: str = "deepseek-chat",
        max_tokens: int = 4000,
        temperature: float = 0.1,
        enable_cache_monitoring: bool = True
    ):
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        self.base_url = base_url or os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com/v1")
        self.model = model
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.enable_cache_monitoring = enable_cache_monitoring
        
        # ç¼“å­˜ç»Ÿè®¡
        self.cache_stats = {
            "total_requests": 0,
            "total_prompt_tokens": 0,
            "cache_hit_tokens": 0,
            "cache_miss_tokens": 0,
            "total_cost_saved": 0.0
        }
        
        if not self.api_key:
            raise ValueError("DeepSeek API key is required. Set DEEPSEEK_API_KEY environment variable.")
        
        # åˆ›å»ºhttpxå®¢æˆ·ç«¯ï¼Œç¦ç”¨ä»£ç†ä»¥é¿å…è¿æ¥é—®é¢˜
        try:
            import httpx
            # é€šè¿‡ç¯å¢ƒå˜é‡æ–¹å¼ç¦ç”¨ä»£ç†
            if 'ALL_PROXY' in os.environ:
                # ä¸´æ—¶ç§»é™¤ä»£ç†ç¯å¢ƒå˜é‡
                old_proxy = os.environ.pop('ALL_PROXY', None)
            http_client = httpx.Client(timeout=30.0)
        except ImportError:
            # å¦‚æœhttpxä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            http_client = None
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼ˆå…¼å®¹DeepSeek APIï¼‰
        if http_client:
            self.client = OpenAI(
                api_key=self.api_key,
                base_url=self.base_url,
                http_client=http_client
            )
        else:
            self.client = OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
    
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stop_sequences: Optional[List[str]] = None
    ) -> Tuple[str, Dict[str, Any]]:
        """å‘é€èŠå¤©å®Œæˆè¯·æ±‚åˆ°DeepSeek APIï¼Œè¿”å›å†…å®¹å’Œä½¿ç”¨ç»Ÿè®¡"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,  # type: ignore
                temperature=temperature or self.temperature,
                max_tokens=max_tokens or self.max_tokens,
                stop=stop_sequences
            )
            
            content = response.choices[0].message.content
            result_content = content.strip() if content else ""
            
            # æå–ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯
            usage_info = {}
            if hasattr(response, 'usage') and response.usage:
                usage_info = {
                    "prompt_tokens": getattr(response.usage, 'prompt_tokens', 0),
                    "completion_tokens": getattr(response.usage, 'completion_tokens', 0),
                    "total_tokens": getattr(response.usage, 'total_tokens', 0),
                    "prompt_cache_hit_tokens": getattr(response.usage, 'prompt_cache_hit_tokens', 0),
                    "prompt_cache_miss_tokens": getattr(response.usage, 'prompt_cache_miss_tokens', 0)
                }
                
                # æ›´æ–°ç¼“å­˜ç»Ÿè®¡
                if self.enable_cache_monitoring:
                    self._update_cache_stats(usage_info)
            
            return result_content, usage_info
        
        except Exception as e:
            raise Exception(f"DeepSeek API è¯·æ±‚å¤±è´¥: {str(e)}")
    
    def _update_cache_stats(self, usage_info: Dict[str, Any]):
        """æ›´æ–°ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        self.cache_stats["total_requests"] += 1
        self.cache_stats["total_prompt_tokens"] += usage_info.get("prompt_tokens", 0)
        
        cache_hit = usage_info.get("prompt_cache_hit_tokens", 0)
        cache_miss = usage_info.get("prompt_cache_miss_tokens", 0)
        
        self.cache_stats["cache_hit_tokens"] += cache_hit
        self.cache_stats["cache_miss_tokens"] += cache_miss
        
        # è®¡ç®—èŠ‚çœçš„æˆæœ¬ (å‡è®¾æ­£å¸¸ä»·æ ¼ä¸º $0.14/M tokensï¼Œç¼“å­˜ä»·æ ¼ä¸º $0.014/M tokens)
        if cache_hit > 0:
            cost_saved = (cache_hit / 1_000_000) * (0.14 - 0.014)
            self.cache_stats["total_cost_saved"] += cost_saved
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if not self.enable_cache_monitoring:
            return {"message": "ç¼“å­˜ç›‘æ§æœªå¯ç”¨"}
        
        stats = self.cache_stats.copy()
        
        # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
        total_cache_tokens = stats["cache_hit_tokens"] + stats["cache_miss_tokens"]
        if total_cache_tokens > 0:
            cache_hit_rate = (stats["cache_hit_tokens"] / total_cache_tokens) * 100
            stats["cache_hit_rate"] = round(cache_hit_rate, 2)
        else:
            stats["cache_hit_rate"] = 0
        
        # æ ¼å¼åŒ–æˆæœ¬èŠ‚çœ
        stats["total_cost_saved"] = round(stats["total_cost_saved"], 4)
        
        return stats
    
    def print_cache_stats(self):
        """æ‰“å°ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_cache_stats()
        
        if "message" in stats:
            print(stats["message"])
            return
        
        print("\nğŸ“Š DeepSeek Context Caching ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 50)
        print(f"ğŸ“ˆ æ€»è¯·æ±‚æ¬¡æ•°: {stats['total_requests']}")
        print(f"ğŸ¯ ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate']}%")
        print(f"âœ… ç¼“å­˜å‘½ä¸­ tokens: {stats['cache_hit_tokens']:,}")
        print(f"âŒ ç¼“å­˜æœªå‘½ä¸­ tokens: {stats['cache_miss_tokens']:,}")
        print(f"ğŸ’° é¢„ä¼°èŠ‚çœæˆæœ¬: ${stats['total_cost_saved']}")
        print("=" * 50)
        
        if stats['cache_hit_tokens'] > 0:
            print("ğŸ‰ æ‚¨æ­£åœ¨å—ç›ŠäºDeepSeekçš„Context CachingåŠŸèƒ½ï¼")
        else:
            print("ğŸ’¡ æç¤ºï¼šå½“æœ‰é‡å¤å†…å®¹æ—¶ï¼Œç¼“å­˜åŠŸèƒ½ä¼šè‡ªåŠ¨æ¿€æ´»")
    
    def generate_response(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """ç”Ÿæˆå•ä¸ªå“åº”ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼Œåªè¿”å›å†…å®¹ï¼‰"""
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        messages.append({"role": "user", "content": prompt})
        
        content, _ = self.chat_completion(messages)
        return content 