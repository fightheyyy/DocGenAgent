"""
ChromaDB RAGå·¥å…· - é‡æ–°è®¾è®¡ç‰ˆ
æ ¸å¿ƒåŠŸèƒ½ï¼šæ–‡æ¡£embeddingå¤„ç†ã€æ™ºèƒ½æœç´¢ã€åŸºäºæ¨¡æ¿å­—æ®µçš„å†…å®¹å¡«å……
"""
import os
import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
import hashlib

try:
    import chromadb
    from chromadb.config import Settings
    import fitz  # PyMuPDF for PDF
    from docx import Document as DocxDocument
except ImportError as e:
    print(f"è­¦å‘Š: RAGå·¥å…·ä¾èµ–æœªå®‰è£…: {e}")
    print("è¯·å®‰è£…: pip install chromadb PyMuPDF python-docx")

try:
    from src.base_tool import Tool
    from src.pdf_embedding_service import PDFEmbeddingService
    PDF_EMBEDDING_SERVICE_AVAILABLE = True
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥Toolï¼Œåˆ›å»ºä¸€ä¸ªåŸºç¡€ç±»
    class Tool:
        def __init__(self):
            self.name = "base_tool"
            self.description = "åŸºç¡€å·¥å…·ç±»"
        
        def execute(self, action: str, **kwargs) -> str:
            return "åŸºç¡€å·¥å…·æ‰§è¡Œ"
    
    PDF_EMBEDDING_SERVICE_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DocumentExtractor:
    """æ–‡æ¡£å†…å®¹æå–å™¨"""
    
    def extract_content(self, file_path: str) -> str:
        """æå–æ–‡æ¡£å†…å®¹"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        file_ext = os.path.splitext(file_path)[1].lower()
        
        try:
            if file_ext == '.pdf':
                return self._extract_from_pdf(file_path)
            elif file_ext == '.docx':
                return self._extract_from_docx(file_path)
            elif file_ext in ['.txt', '.md']:
                return self._extract_from_text(file_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
        except Exception as e:
            raise RuntimeError(f"æ–‡æ¡£å†…å®¹æå–å¤±è´¥: {str(e)}")
    
    def _extract_from_pdf(self, file_path: str) -> str:
        """ä»PDFæå–å†…å®¹"""
        content = ""
        doc = fitz.open(file_path)
        for page in doc:
            content += page.get_text()
        doc.close()
        return content.strip()
    
    def _extract_from_docx(self, file_path: str) -> str:
        """ä»DOCXæå–å†…å®¹"""
        doc = DocxDocument(file_path)
        content = []
        
        # æå–æ®µè½å†…å®¹
        for para in doc.paragraphs:
            if para.text.strip():
                content.append(para.text.strip())
        
        # æå–è¡¨æ ¼å†…å®¹
        for table in doc.tables:
            for row in table.rows:
                row_text = " | ".join([cell.text.strip() for cell in row.cells])
                if row_text.strip():
                    content.append(f"è¡¨æ ¼è¡Œ: {row_text}")
        
        return "\n".join(content)
    
    def _extract_from_text(self, file_path: str) -> str:
        """ä»æ–‡æœ¬æ–‡ä»¶æå–å†…å®¹"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read().strip()

class ChromaVectorStore:
    """ChromaDBå‘é‡å­˜å‚¨"""
    
    def __init__(self, storage_dir: str):
        self.storage_dir = storage_dir
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                self.client = chromadb.PersistentClient(
                    path=storage_dir,
                    settings=Settings(
                        allow_reset=True, 
                        anonymized_telemetry=False,
                        is_persistent=True
                    )
                )
                self.collection = self.client.get_or_create_collection(
                    name="documents",
                    metadata={"hnsw:space": "cosine"}
                )
                logger.info(f"âœ… ChromaVectorStoreåˆå§‹åŒ–æˆåŠŸ: {storage_dir}")
                return
                
            except Exception as e:
                error_msg = str(e)
                if "already exists" in error_msg and "different settings" in error_msg:
                    logger.warning(f"âš ï¸ ChromaDBå®ä¾‹å†²çªï¼Œå°è¯•é‡ç½®... (å°è¯• {attempt + 1}/{max_retries})")
                    
                    # å°è¯•é‡ç½®ChromaDBè¿æ¥
                    try:
                        if hasattr(self, 'client') and self.client:
                            self.client.reset()
                    except:
                        pass
                    
                    # ç­‰å¾…ä¸€ä¸‹å†é‡è¯•
                    import time
                    time.sleep(1)
                    
                    if attempt == max_retries - 1:
                        # æœ€åä¸€æ¬¡å°è¯•ï¼šåˆ é™¤å¹¶é‡æ–°åˆ›å»º
                        try:
                            import shutil
                            if os.path.exists(storage_dir):
                                logger.info(f"ğŸ”„ æ¸…ç†ChromaDBç›®å½•: {storage_dir}")
                                shutil.rmtree(storage_dir)
                                os.makedirs(storage_dir, exist_ok=True)
                        except Exception as cleanup_error:
                            logger.warning(f"âš ï¸ æ¸…ç†å¤±è´¥: {cleanup_error}")
                else:
                    logger.error(f"âŒ ChromaVectorStoreåˆå§‹åŒ–å¤±è´¥: {e}")
                    if attempt == max_retries - 1:
                        raise
                    else:
                        import time
                        time.sleep(1)
        
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        raise RuntimeError("ChromaVectorStoreåˆå§‹åŒ–å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
    
    def add_document(self, doc_id: str, content: str, metadata: Dict[str, Any]):
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡åº“"""
        # å°†é•¿æ–‡æ¡£åˆ†å—
        chunks = self._split_content(content)
        
        ids = []
        documents = []
        metadatas = []
        
        for i, chunk in enumerate(chunks):
            chunk_id = f"{doc_id}_chunk_{i}"
            ids.append(chunk_id)
            documents.append(chunk)
            chunk_metadata = metadata.copy()
            chunk_metadata.update({
                "chunk_index": i,
                "total_chunks": len(chunks)
            })
            metadatas.append(chunk_metadata)
        
        self.collection.add(
            ids=ids,
            documents=documents,
            metadatas=metadatas
        )
        
        return len(chunks)
    
    def search_documents(self, query: str, n_results: int = 5, where_filter: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸å…³æ–‡æ¡£ï¼Œæ”¯æŒå…ƒæ•°æ®è¿‡æ»¤"""
        query_params = {
            "query_texts": [query],
            "n_results": n_results
        }
        if where_filter:
            query_params["where"] = where_filter
            logger.info(f"ğŸ” ä½¿ç”¨å…ƒæ•°æ®è¿‡æ»¤å™¨è¿›è¡Œæœç´¢: {where_filter}")
            
        results = self.collection.query(**query_params)
        
        formatted_results = []
        if results['documents'] and len(results['documents']) > 0:
            for i in range(len(results['documents'][0])):
                formatted_results.append({
                    'content': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'distance': results['distances'][0][i]
                })
        
        return formatted_results
    
    def get_all_documents(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ–‡æ¡£ä¿¡æ¯"""
        try:
            results = self.collection.get()
            documents = []
            
            # æŒ‰æ–‡æ¡£IDåˆ†ç»„
            doc_groups = {}
            for i, doc_id in enumerate(results['ids']):
                base_id = doc_id.split('_chunk_')[0]
                if base_id not in doc_groups:
                    doc_groups[base_id] = {
                        'id': base_id,
                        'metadata': results['metadatas'][i],
                        'chunks': 0
                    }
                doc_groups[base_id]['chunks'] += 1
            
            return list(doc_groups.values())
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def clear_all(self):
        """æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£"""
        self.client.delete_collection("documents")
        self.collection = self.client.get_or_create_collection(
            name="documents",
            metadata={"hnsw:space": "cosine"}
        )
    
    def _split_content(self, content: str, chunk_size: int = 1000) -> List[str]:
        """å°†å†…å®¹åˆ†å—"""
        if len(content) <= chunk_size:
            return [content]
        
        chunks = []
        sentences = content.split('ã€‚')
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence + 'ã€‚') <= chunk_size:
                current_chunk += sentence + 'ã€‚'
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + 'ã€‚'
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks if chunks else [content]

class TemplateFieldProcessor:
    """æ¨¡æ¿å­—æ®µå¤„ç†å™¨ - æ ¸å¿ƒæ™ºèƒ½å¡«å……åŠŸèƒ½"""
    
    def __init__(self, deepseek_client=None):
        self.deepseek_client = deepseek_client
    
    def fill_template_fields(self, template_fields_json: Dict[str, str], 
                           vector_store: ChromaVectorStore) -> Dict[str, str]:
        """
        åŸºäºæ¨¡æ¿å­—æ®µJSONè¿›è¡Œæ™ºèƒ½æœç´¢å’Œå†…å®¹å¡«å……
        
        Args:
            template_fields_json: æ¨¡æ¿å­—æ®µJSONï¼Œæ ¼å¼ä¸º {"å­—æ®µå": "å­—æ®µæè¿°æˆ–è¦æ±‚"}
            vector_store: å‘é‡å­˜å‚¨å®ä¾‹
            
        Returns:
            å¡«å……å¥½çš„å­—æ®µJSONï¼Œæ ¼å¼ä¸º {"å­—æ®µå": "å¡«å……çš„å…·ä½“å†…å®¹"}
        """
        logger.info(f"ğŸ” å¼€å§‹åŸºäºæ¨¡æ¿å­—æ®µè¿›è¡Œæ™ºèƒ½å¡«å……ï¼Œå…± {len(template_fields_json)} ä¸ªå­—æ®µ")
        
        filled_fields = {}
        
        for field_name, field_requirement in template_fields_json.items():
            logger.info(f"ğŸ“ å¤„ç†å­—æ®µ: {field_name}")
            
            # 1. åŸºäºå­—æ®µè¦æ±‚æœç´¢ç›¸å…³å†…å®¹
            search_results = vector_store.search_documents(
                query=f"{field_name} {field_requirement}",
                n_results=3
            )
            
            # 2. æå–æœç´¢åˆ°çš„å†…å®¹
            relevant_content = []
            for result in search_results:
                relevant_content.append(result['content'])
            
            # 3. ä½¿ç”¨AIç”Ÿæˆå­—æ®µå†…å®¹ï¼ˆå¦‚æœæœ‰AIå®¢æˆ·ç«¯ï¼‰
            if self.deepseek_client and relevant_content:
                filled_content = self._generate_field_content_with_ai(
                    field_name, field_requirement, relevant_content
                )
            else:
                # åŸºç¡€æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨æœç´¢åˆ°çš„æœ€ç›¸å…³å†…å®¹
                filled_content = self._generate_field_content_basic(
                    field_name, field_requirement, relevant_content
                )
            
            filled_fields[field_name] = filled_content
            logger.info(f"âœ… å­—æ®µ {field_name} å¡«å……å®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(filled_content)} å­—ç¬¦")
        
        logger.info(f"ğŸ‰ æ‰€æœ‰å­—æ®µå¡«å……å®Œæˆï¼")
        return filled_fields
    
    def _generate_field_content_with_ai(self, field_name: str, field_requirement: str, 
                                       relevant_content: List[str]) -> str:
        """ä½¿ç”¨AIç”Ÿæˆå­—æ®µå†…å®¹"""
        content_text = "\n".join(relevant_content)
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£å¤„ç†åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ä¸ºå­—æ®µç”Ÿæˆåˆé€‚çš„å†…å®¹ï¼š

å­—æ®µåç§°ï¼š{field_name}
å­—æ®µè¦æ±‚ï¼š{field_requirement}

ç›¸å…³èµ„æ–™å†…å®¹ï¼š
{content_text}

ä»»åŠ¡è¦æ±‚ï¼š
1. åŸºäºç›¸å…³èµ„æ–™å†…å®¹ï¼Œä¸ºè¯¥å­—æ®µç”Ÿæˆä¸“ä¸šã€å‡†ç¡®çš„å†…å®¹
2. å†…å®¹åº”è¯¥ç¬¦åˆå­—æ®µè¦æ±‚å’Œæè¿°
3. ä¿æŒå†…å®¹çš„ä¸“ä¸šæ€§å’Œå®Œæ•´æ€§
4. å¦‚æœèµ„æ–™å†…å®¹ä¸è¶³ï¼Œè¯·åŸºäºå­—æ®µè¦æ±‚è¿›è¡Œåˆç†è¡¥å……
5. å†…å®¹é•¿åº¦é€‚ä¸­ï¼Œé‡ç‚¹çªå‡º

è¯·ç›´æ¥è¿”å›è¯¥å­—æ®µçš„å…·ä½“å†…å®¹ï¼Œä¸è¦åŒ…å«è§£é‡Šæ–‡å­—ã€‚
"""
        
        try:
            response = self.deepseek_client.chat([{"role": "user", "content": prompt}])
            return response.strip() if response else self._generate_field_content_basic(
                field_name, field_requirement, relevant_content
            )
        except Exception as e:
            logger.warning(f"AIç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å¼: {e}")
            return self._generate_field_content_basic(
                field_name, field_requirement, relevant_content
            )
    
    def _generate_field_content_basic(self, field_name: str, field_requirement: str, 
                                     relevant_content: List[str]) -> str:
        """åŸºç¡€æ¨¡å¼ç”Ÿæˆå­—æ®µå†…å®¹"""
        if not relevant_content:
            return f"[{field_name}]ï¼š{field_requirement}ï¼ˆå¾…è¡¥å……å…·ä½“å†…å®¹ï¼‰"
        
        # é€‰æ‹©æœ€ç›¸å…³çš„å†…å®¹ä½œä¸ºåŸºç¡€
        base_content = relevant_content[0]
        
        # ç®€å•çš„å†…å®¹å¤„ç†
        if len(base_content) > 200:
            # æˆªå–å‰200å­—ç¬¦ä½œä¸ºæ‘˜è¦
            summary = base_content[:200] + "..."
            return f"{summary}\n\nï¼ˆåŸºäºç›¸å…³èµ„æ–™æ•´ç†ï¼Œå¦‚éœ€è¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒåŸå§‹æ–‡æ¡£ï¼‰"
        else:
            return base_content

class RAGTool(Tool):
    """é‡æ–°è®¾è®¡çš„RAGå·¥å…· - æ ¸å¿ƒä¸‰åŠŸèƒ½"""
    
    def __init__(self, storage_dir: str = "rag_storage", deepseek_client=None):
        super().__init__()
        self.name = "rag_tool"
        self.description = """æ™ºèƒ½æ–‡æ¡£æ£€ç´¢å·¥å…· - æ”¯æŒæ–‡æœ¬ã€å›¾ç‰‡ã€è¡¨æ ¼çš„ç²¾ç¡®æœç´¢

ğŸ” **æ”¯æŒçš„æ“ä½œ (action):**

1. **search** - é€šç”¨æœç´¢ï¼ˆè¿”å›æ··åˆå†…å®¹ï¼‰
   å‚æ•°: {"action": "search", "query": "å…³é”®è¯", "top_k": 5}

2. **search_images** - ä¸“é—¨æœç´¢å›¾ç‰‡ ğŸ–¼ï¸
   å‚æ•°: {"action": "search_images", "query": "å…³é”®è¯", "top_k": 5}
   ç¤ºä¾‹: {"action": "search_images", "query": "åŒ»çµå¤åº™", "top_k": 8}

3. **search_tables** - ä¸“é—¨æœç´¢è¡¨æ ¼ ğŸ“Š
   å‚æ•°: {"action": "search_tables", "query": "å…³é”®è¯", "top_k": 5}
   ç¤ºä¾‹: {"action": "search_tables", "query": "å½±å“è¯„ä¼°", "top_k": 5}

4. **count_images** - ç»Ÿè®¡å›¾ç‰‡æ•°é‡ ğŸ“ˆ
   å‚æ•°: {"action": "count_images", "query": "å…³é”®è¯"}
   ç¤ºä¾‹: {"action": "count_images", "query": "åŒ»çµå¤åº™"}

5. **count_tables** - ç»Ÿè®¡è¡¨æ ¼æ•°é‡ ğŸ“ˆ
   å‚æ•°: {"action": "count_tables", "query": "å…³é”®è¯"}
   ç¤ºä¾‹: {"action": "count_tables", "query": "è¯„ä¼°æ ‡å‡†"}

6. **list** - åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£
   å‚æ•°: {"action": "list"}

7. **process_parsed_folder** - å¤„ç†è§£ææ–‡ä»¶å¤¹
   å‚æ•°: {"action": "process_parsed_folder", "folder_path": "è·¯å¾„"}

âš ï¸ **é‡è¦æç¤º:**
- æœç´¢å›¾ç‰‡è¯·ä½¿ç”¨ search_imagesï¼Œä¸è¦ä½¿ç”¨ search + search_type
- ç»Ÿè®¡æ•°é‡è¯·ä½¿ç”¨ count_images/count_tablesï¼Œä¸è¦ä½¿ç”¨ limit å‚æ•°
- top_k å‚æ•°æ§åˆ¶è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤5ï¼‰
- æ‰€æœ‰æ“ä½œéƒ½éœ€è¦æ˜ç¡®æŒ‡å®š action å‚æ•°

ğŸ’¡ **ä½¿ç”¨åœºæ™¯:**
- é—®"æœ‰å¤šå°‘å¼ å›¾ç‰‡ï¼Ÿ" â†’ ä½¿ç”¨ count_images
- é—®"æ£€ç´¢Nå¼ å›¾ç‰‡" â†’ ä½¿ç”¨ search_images + top_k
- é—®"æœç´¢è¡¨æ ¼æ•°æ®" â†’ ä½¿ç”¨ search_tables
"""
        
        self.storage_dir = storage_dir
        os.makedirs(storage_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.extractor = DocumentExtractor()
        self.vector_store = ChromaVectorStore(storage_dir)
        self.field_processor = TemplateFieldProcessor(deepseek_client)
        
        # åˆå§‹åŒ–æ”¹è¿›çš„PDF embeddingæœåŠ¡
        self.pdf_embedding_service = None
        if PDF_EMBEDDING_SERVICE_AVAILABLE:
            try:
                self.pdf_embedding_service = PDFEmbeddingService(
                    chroma_db_path=storage_dir,
                    collection_name="documents"
                )
                logger.info("âœ… PDF Embedding Service integrated successfully")
            except Exception as e:
                logger.warning(f"âš ï¸ PDF Embedding Service initialization failed: {e}")
        else:
            logger.warning("âš ï¸ PDF Embedding Service not available")
    
    def execute(self, action: str, **kwargs) -> str:
        """æ‰§è¡ŒRAGæ“ä½œ"""
        try:
            if action == "upload":
                return self._upload_document(**kwargs)
            elif action == "upload_image":
                return self._upload_image(**kwargs)
            elif action == "search":
                return self._search_documents(**kwargs)
            elif action == "search_images":
                return self._search_images(**kwargs)
            elif action == "search_tables":
                return self._search_tables(**kwargs)
            elif action == "count_images":
                return self._count_images(**kwargs)
            elif action == "count_tables":
                return self._count_tables(**kwargs)
            elif action == "fill_fields":
                return self._fill_template_fields(**kwargs)
            elif action == "list":
                return self._list_documents()
            elif action == "clear":
                return self._clear_documents()
            elif action == "process_parsed_folder":
                folder_path = kwargs.get("folder_path")
                project_name = kwargs.get("project_name", "")
                if not folder_path:
                    return "âŒ è¯·æä¾›è§£ææ–‡ä»¶å¤¹è·¯å¾„ (folder_pathå‚æ•°)"
                return self._process_parsed_folder(folder_path, project_name)
            else:
                return f"""âŒ ä¸æ”¯æŒçš„æ“ä½œ: {action}

ğŸ“‹ **æ”¯æŒçš„æ“ä½œåˆ—è¡¨:**
â€¢ upload - ä¸Šä¼ æ–‡æ¡£
â€¢ upload_image - ä¸Šä¼ å›¾ç‰‡å¹¶ç”ŸæˆAIæè¿°
â€¢ search - é€šç”¨æœç´¢
â€¢ search_images - æœç´¢å›¾ç‰‡
â€¢ search_tables - æœç´¢è¡¨æ ¼  
â€¢ count_images - ç»Ÿè®¡å›¾ç‰‡æ•°é‡
â€¢ count_tables - ç»Ÿè®¡è¡¨æ ¼æ•°é‡
â€¢ list - åˆ—å‡ºæ–‡æ¡£
â€¢ process_parsed_folder - å¤„ç†è§£ææ–‡ä»¶å¤¹

ğŸ’¡ **ä½¿ç”¨ç¤ºä¾‹:**
â€¢ ä¸Šä¼ æ–‡æ¡£: {{"action": "upload", "file_path": "document.pdf"}}
â€¢ ä¸Šä¼ å›¾ç‰‡: {{"action": "upload_image", "image_path": "image.jpg", "description": "å¯é€‰æè¿°"}}
â€¢ æœç´¢å›¾ç‰‡: {{"action": "search_images", "query": "åŒ»çµå¤åº™", "top_k": 8}}
â€¢ ç»Ÿè®¡å›¾ç‰‡: {{"action": "count_images", "query": "åŒ»çµå¤åº™"}}
"""
        
        except Exception as e:
            error_msg = str(e)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å‚æ•°é”™è¯¯
            if "unexpected keyword argument" in error_msg:
                if "search_type" in error_msg:
                    return """âŒ å‚æ•°é”™è¯¯: searchæ“ä½œä¸æ”¯æŒsearch_typeå‚æ•°

âœ… **æ­£ç¡®åšæ³•:**
â€¢ æœç´¢å›¾ç‰‡: {"action": "search_images", "query": "å…³é”®è¯"}
â€¢ æœç´¢è¡¨æ ¼: {"action": "search_tables", "query": "å…³é”®è¯"}
â€¢ ç»Ÿè®¡å›¾ç‰‡: {"action": "count_images", "query": "å…³é”®è¯"}
"""
                elif "limit" in error_msg:
                    return """âŒ å‚æ•°é”™è¯¯: ä¸æ”¯æŒlimitå‚æ•°

âœ… **æ­£ç¡®åšæ³•:**
â€¢ ä½¿ç”¨top_kå‚æ•°: {"action": "search_images", "query": "å…³é”®è¯", "top_k": 8}
â€¢ æˆ–ä½¿ç”¨é»˜è®¤å€¼: {"action": "search_images", "query": "å…³é”®è¯"}
"""
            
            logger.error(f"RAGæ“ä½œå¤±è´¥: {e}")
            return f"âŒ æ“ä½œå¤±è´¥: {str(e)}"
    
    def _upload_document(self, file_path: str, filename: str = "", metadata: Optional[Dict[str, Any]] = None) -> str:
        """
        ä¸Šä¼ å¹¶å¤„ç†å•ä¸ªæ–‡æ¡£
        
        Args:
            file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
            filename: æ–‡æ¡£åœ¨ç³»ç»Ÿä¸­çš„åç§°ï¼ˆå¯é€‰ï¼‰
            metadata: é™„åŠ çš„å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å¤„ç†ç»“æœ
        """
        try:
            doc_name = filename if filename else os.path.basename(file_path)
            doc_id = hashlib.md5(doc_name.encode()).hexdigest()
            
            logger.info(f"ğŸ“¤ å¼€å§‹å¤„ç†æ–‡æ¡£: {doc_name}")
            
            # 1. æå–å†…å®¹
            content = self.extractor.extract_content(file_path)
            
            # 2. å‡†å¤‡å…ƒæ•°æ®
            if metadata is None:
                metadata = {}
            
            # ç¡®ä¿åŸºæœ¬å…ƒæ•°æ®å­˜åœ¨
            metadata.setdefault("source", doc_name)
            metadata.setdefault("upload_time", datetime.now().isoformat())
            metadata.setdefault("file_size", os.path.getsize(file_path))

            # 3. æ·»åŠ åˆ°å‘é‡åº“
            chunks_count = self.vector_store.add_document(doc_id, content, metadata)
            
            logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {doc_name} (ID: {doc_id}), å…± {chunks_count} ä¸ªå—")
            
            return f"âœ… æ–‡æ¡£ '{doc_name}' ä¸Šä¼ å¹¶å¤„ç†æˆåŠŸï¼Œå…±åˆ†ä¸º {chunks_count} ä¸ªå†…å®¹å—ã€‚"
        except FileNotFoundError:
            return f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}"
        except Exception as e:
            return f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}"
    
    def _upload_image(self, image_path: str, description: str = "") -> str:
        """
        ä¸Šä¼ å›¾ç‰‡ï¼Œç”ŸæˆAIæè¿°ï¼Œå¹¶å°†å…¶åµŒå…¥åˆ°ç»Ÿä¸€çŸ¥è¯†åº“
        
        Args:
            image_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„
            description: ç”¨æˆ·æä¾›çš„å¯é€‰æè¿°
            
        Returns:
            å¤„ç†ç»“æœçš„JSONå­—ç¬¦ä¸²
        """
        try:
            from pathlib import Path
            
            if not image_path or not os.path.exists(image_path):
                return json.dumps({"status": "error", "message": "å›¾ç‰‡è·¯å¾„ä¸å­˜åœ¨æˆ–æœªæä¾›"}, ensure_ascii=False)
            
            # å¯¼å…¥OpenRouterå®¢æˆ·ç«¯ç”¨äºAIæè¿°ç”Ÿæˆ
            try:
                from src.openrouter_client import OpenRouterClient
                openrouter_client = OpenRouterClient()
            except Exception as e:
                return json.dumps({"status": "error", "message": f"AIæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}"}, ensure_ascii=False)
            
            logger.info(f"ğŸš€ å¼€å§‹å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡: {image_path}")
            
            # 1. ä½¿ç”¨Geminiç”Ÿæˆå›¾ç‰‡æè¿°
            prompt = "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬åœºæ™¯ã€ç‰©ä½“ã€äººç‰©ã€é£æ ¼å’Œä»»ä½•å¯è§çš„æ–‡æœ¬ã€‚"
            ai_description = openrouter_client.get_image_description_gemini(image_path, prompt=prompt)
            
            if "Error:" in ai_description:
                raise Exception(f"AIæè¿°ç”Ÿæˆå¤±è´¥: {ai_description}")

            final_description = f"ç”¨æˆ·æè¿°: {description}\n\nAIæè¿°: {ai_description}" if description else ai_description
            logger.info(f"ğŸ“ ç”Ÿæˆçš„æè¿°: {final_description[:150]}...")

            # 2. å‡†å¤‡å…ƒæ•°æ®å¹¶åµŒå…¥åˆ°ChromaDB
            image_name = Path(image_path).name
            doc_id = hashlib.md5(image_name.encode()).hexdigest()
            
            metadata = {
                "source": image_name,
                "document_type": "Image",
                "upload_time": datetime.now().isoformat(),
                "user_provided_description": bool(description),
                "file_size": os.path.getsize(image_path)
            }
            
            # 3. æ·»åŠ åˆ°å‘é‡åº“
            chunks_count = self.vector_store.add_document(doc_id, final_description, metadata)
            
            logger.info(f"âœ… å›¾ç‰‡å¤„ç†å®Œæˆ: {image_name} (ID: {doc_id}), å…± {chunks_count} ä¸ªå—")
            
            result = {
                "status": "success",
                "message": "å›¾ç‰‡å¤„ç†å’ŒåµŒå…¥æˆåŠŸ",
                "image_source": image_path,
                "chunks_count": chunks_count,
                "generated_description": final_description
            }
            
            return json.dumps(result, ensure_ascii=False, indent=2)

        except Exception as e:
            logger.error(f"âŒ å›¾ç‰‡ä¸Šä¼ å’ŒåµŒå…¥æµç¨‹å¤±è´¥: {e}")
            return json.dumps({"status": "error", "message": str(e)}, ensure_ascii=False)
    
    def _search_documents(self, query: str, top_k: int = 5, metadata_filter: Optional[Dict[str, Any]] = None, project_name: Optional[str] = None) -> str:
        """
        æ ¹æ®æŸ¥è¯¢æ–‡æœ¬æœç´¢æ–‡æ¡£ - æ”¯æŒæ™ºèƒ½é¡¹ç›®éš”ç¦»
        
        Args:
            query (str): æœç´¢çš„å…³é”®è¯æˆ–é—®é¢˜ã€‚
            top_k (int): è¿”å›æœ€ç›¸å…³çš„ç»“æœæ•°é‡ï¼Œé»˜è®¤ä¸º5ã€‚
            metadata_filter (dict, optional): ç”¨äºç²¾ç¡®è¿‡æ»¤çš„å…ƒæ•°æ®ã€‚
            project_name (str, optional): é¡¹ç›®åç§°è¿‡æ»¤å™¨ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›ä¼šå°è¯•è‡ªåŠ¨æå–ï¼‰
                *   ç¤ºä¾‹1 (åªæœå›¾ç‰‡): `{"document_type": "Image"}`
                *   ç¤ºä¾‹2 (åªæœæ–‡æœ¬): `{"document_type": "Text"}`
                *   ç¤ºä¾‹3 (åªæœç‰¹å®šæ–‡ä»¶): `{"source_document": "your_file.pdf"}`
                *   ç¤ºä¾‹4 (å¤åˆæŸ¥è¯¢ï¼šåªæœç‰¹å®šæ–‡ä»¶ä¸­çš„å›¾ç‰‡): `{"$and": [{"document_type": "Image"}, {"source_document": "your_file.pdf"}]}`
            
        Returns:
            str: åŒ…å«æœç´¢ç»“æœçš„JSONå­—ç¬¦ä¸²
        """
        logger.info(f"æ‰§è¡Œæ–‡æ¡£æœç´¢: query='{query}', top_k={top_k}, filter={metadata_filter}")
        
        try:
            # ğŸ†• æ™ºèƒ½é¡¹ç›®åç§°æå–
            if not project_name:
                project_name = self._extract_project_name_from_query(query)
                if project_name:
                    logger.info(f"ğŸ¯ è‡ªåŠ¨æå–é¡¹ç›®åç§°: {project_name}")
            
            # ğŸ†• å¦‚æœæœ‰é¡¹ç›®åç§°ä¸”PDF Embedding Serviceå¯ç”¨ï¼Œä¼˜å…ˆä½¿ç”¨é¡¹ç›®éš”ç¦»æœç´¢
            if project_name and self.pdf_embedding_service:
                logger.info(f"ğŸ” ä½¿ç”¨é¡¹ç›®éš”ç¦»æœç´¢: {project_name}")
                results = self.pdf_embedding_service.search(
                    query=query,
                    top_k=top_k,
                    content_type=None,  # æœç´¢æ‰€æœ‰ç±»å‹
                    source_file_filter=None,
                    project_name=project_name
                )
                
                simplified_results = []
                for res in results:
                    simplified_results.append({
                        "content": res.get("content", ""),
                        "metadata": res.get("metadata", {}),
                        "distance": res.get("distance", 0.0)
                    })
                
                result_data = {
                    "status": "success", 
                    "results": simplified_results,
                    "total_count": len(simplified_results),
                    "search_method": "pdf_embedding_service_with_project_isolation"
                }
                
                result_data["project_isolation"] = {
                    "enabled": True,
                    "project_name": project_name,
                    "message": f"ğŸ”’ å·²é™åˆ¶æœç´¢èŒƒå›´è‡³é¡¹ç›®: {project_name}"
                }
                
                return json.dumps(result_data, ensure_ascii=False)
            
            # ğŸ”„ å›é€€åˆ°ä¼ ç»Ÿå‘é‡æœç´¢
            else:
                logger.info("ğŸ” ä½¿ç”¨ä¼ ç»Ÿå‘é‡æœç´¢")
                results = self.vector_store.search_documents(
                    query=query, 
                    n_results=top_k,
                    where_filter=metadata_filter
                )
                
                # ç®€åŒ–è¾“å‡º
                simplified_results = []
                for res in results:
                    simplified_results.append({
                        "content": res.get("content"),
                        "metadata": res.get("metadata"),
                        "distance": res.get("distance")
                    })

                result_data = {
                    "status": "success", 
                    "results": simplified_results,
                    "total_count": len(simplified_results),
                    "search_method": "vector_store"
                }
                
                if project_name:
                    result_data["project_isolation"] = {
                        "enabled": False,
                        "attempted_project": project_name,
                        "message": f"âš ï¸ PDF Embedding Serviceä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæœç´¢ï¼ˆæ— é¡¹ç›®éš”ç¦»ï¼‰"
                    }
                else:
                    result_data["project_isolation"] = {
                        "enabled": False,
                        "message": "âš ï¸ æœªå¯ç”¨é¡¹ç›®éš”ç¦»ï¼Œæœç´¢äº†æ‰€æœ‰é¡¹ç›®"
                    }

                return json.dumps(result_data, ensure_ascii=False)
                
        except Exception as e:
            logger.error(f"æ–‡æ¡£æœç´¢å¤±è´¥: {e}")
            return json.dumps({"status": "error", "message": str(e)})
    
    def _search_images(self, query: str, top_k: int = 5, source_file_filter: Optional[str] = None, project_name: Optional[str] = None) -> str:
        """
        æœç´¢å›¾ç‰‡å†…å®¹ - æ”¯æŒæ™ºèƒ½é¡¹ç›®éš”ç¦»
        
        Args:
            query: æœç´¢å…³é”®è¯
            top_k: è¿”å›ç»“æœæ•°é‡
            source_file_filter: æºæ–‡ä»¶è¿‡æ»¤å™¨
            project_name: é¡¹ç›®åç§°è¿‡æ»¤å™¨ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›ä¼šå°è¯•è‡ªåŠ¨æå–ï¼‰
            
        Returns:
            åŒ…å«æœç´¢ç»“æœçš„JSONå­—ç¬¦ä¸²
        """
        if not self.pdf_embedding_service:
            return json.dumps({
                "status": "error", 
                "message": "PDF Embedding Service not available"
            })
        
        try:
            # ğŸ†• æ™ºèƒ½é¡¹ç›®åç§°æå–
            if not project_name:
                # å°è¯•ä»æŸ¥è¯¢ä¸­æå–é¡¹ç›®åç§°
                project_name = self._extract_project_name_from_query(query)
                if project_name:
                    logger.info(f"ğŸ¯ è‡ªåŠ¨æå–é¡¹ç›®åç§°: {project_name}")
            
            results = self.pdf_embedding_service.search_images_only(
                query=query, 
                top_k=top_k, 
                source_file_filter=source_file_filter,
                project_name=project_name  # ğŸ†• é¡¹ç›®éš”ç¦»å‚æ•°
            )
            
            simplified_results = []
            for res in results:
                simplified_results.append({
                    "content": res.get("content", ""),
                    "metadata": res.get("metadata", {}),
                    "content_type": res.get("content_type", "image"),
                    "distance": res.get("distance", 0.0)
                })
            
            # ğŸ†• æ·»åŠ é¡¹ç›®éš”ç¦»ä¿¡æ¯åˆ°è¿”å›ç»“æœ
            result_data = {
                "status": "success", 
                "results": simplified_results,
                "total_count": len(simplified_results)
            }
            
            if project_name:
                result_data["project_isolation"] = {
                    "enabled": True,
                    "project_name": project_name,
                    "message": f"ğŸ”’ å·²é™åˆ¶æœç´¢èŒƒå›´è‡³é¡¹ç›®: {project_name}"
                }
            else:
                result_data["project_isolation"] = {
                    "enabled": False,
                    "message": "âš ï¸ æœªå¯ç”¨é¡¹ç›®éš”ç¦»ï¼Œæœç´¢äº†æ‰€æœ‰é¡¹ç›®"
                }
            
            return json.dumps(result_data, ensure_ascii=False)
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡æœç´¢å¤±è´¥: {e}")
            return json.dumps({"status": "error", "message": str(e)})
    
    def _extract_project_name_from_query(self, query: str) -> Optional[str]:
        """
        ä»æŸ¥è¯¢ä¸­æ™ºèƒ½æå–é¡¹ç›®åç§°
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            
        Returns:
            æå–åˆ°çš„é¡¹ç›®åç§°ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
        """
        try:
            # è·å–æ‰€æœ‰å¯ç”¨é¡¹ç›®
            if hasattr(self.pdf_embedding_service, 'get_available_projects'):
                available_projects = self.pdf_embedding_service.get_available_projects()
                
                # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆåŒ¹é…æ›´é•¿çš„é¡¹ç›®åç§°
                available_projects = sorted(available_projects, key=len, reverse=True)
                
                # ğŸ” æ­£ç¡®çš„åŒ¹é…é€»è¾‘ï¼šæ£€æŸ¥æŸ¥è¯¢æ˜¯å¦åŒ…å«åœ¨é¡¹ç›®åç§°ä¸­
                for project in available_projects:
                    if query in project:
                        return project
                
                # ğŸ” æ™ºèƒ½å…³é”®è¯åŒ¹é…ï¼šæå–é¡¹ç›®çš„æ ¸å¿ƒå…³é”®è¯
                for project in available_projects:
                    # ç§»é™¤å¸¸è§åç¼€ï¼Œæå–æ ¸å¿ƒå…³é”®è¯
                    project_core = project.replace("è®¾è®¡æ–¹æ¡ˆ", "").replace("ä¿®ç¼®è®¾è®¡æ–¹æ¡ˆ", "").replace("é¡¹ç›®", "").replace("æ–‡ç‰©", "").strip()
                    
                    # æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦åŒ…å«æ ¸å¿ƒå…³é”®è¯
                    if project_core and query in project_core:
                        return project
                    
                    # æ£€æŸ¥æ ¸å¿ƒå…³é”®è¯æ˜¯å¦åŒ…å«åœ¨æŸ¥è¯¢ä¸­
                    if project_core and project_core in query:
                        return project
                
                # ğŸ” åˆ†è¯åŒ¹é…ï¼šå¤„ç†å¤åˆè¯æƒ…å†µ
                for project in available_projects:
                    # åˆ†è§£é¡¹ç›®åç§°ä¸ºå…³é”®è¯åˆ—è¡¨
                    project_keywords = []
                    
                    # æå–ä¸»è¦å…³é”®è¯
                    if "å®—ç¥ " in project:
                        project_keywords.extend(["å®—ç¥ "])
                        # æå–å®—ç¥ å‰çš„å§“æ°
                        if "æ°å®—ç¥ " in project:
                            idx = project.find("æ°å®—ç¥ ")
                            if idx > 0:
                                surname = project[idx-1:idx+1]  # å¦‚"åˆ˜æ°"
                                project_keywords.append(surname)
                    
                    if "å¤åº™" in project:
                        project_keywords.extend(["å¤åº™"])
                        # æå–å¤åº™å‰çš„åç§°
                        idx = project.find("å¤åº™")
                        if idx > 0:
                            # å°è¯•æå–2-3ä¸ªå­—ç¬¦çš„åç§°
                            for length in [3, 2]:
                                if idx >= length:
                                    name = project[idx-length:idx]
                                    if name not in ["è®¾è®¡", "ä¿®ç¼®", "æ–¹æ¡ˆ"]:
                                        project_keywords.append(name)
                                        break
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å…³é”®è¯åŒ¹é…
                    for keyword in project_keywords:
                        if keyword in query:
                            return project
            
            return None
            
        except Exception as e:
            logger.warning(f"é¡¹ç›®åç§°æå–å¤±è´¥: {e}")
            return None
    
    def _search_tables(self, query: str, top_k: int = 5, source_file_filter: Optional[str] = None, project_name: Optional[str] = None) -> str:
        """
        æœç´¢è¡¨æ ¼å†…å®¹ - æ”¯æŒæ™ºèƒ½é¡¹ç›®éš”ç¦»
        
        Args:
            query: æœç´¢å…³é”®è¯
            top_k: è¿”å›ç»“æœæ•°é‡
            source_file_filter: æºæ–‡ä»¶è¿‡æ»¤å™¨
            project_name: é¡¹ç›®åç§°è¿‡æ»¤å™¨ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›ä¼šå°è¯•è‡ªåŠ¨æå–ï¼‰
            
        Returns:
            åŒ…å«æœç´¢ç»“æœçš„JSONå­—ç¬¦ä¸²
        """
        if not self.pdf_embedding_service:
            return json.dumps({
                "status": "error", 
                "message": "PDF Embedding Service not available"
            })
        
        try:
            # ğŸ†• æ™ºèƒ½é¡¹ç›®åç§°æå–
            if not project_name:
                project_name = self._extract_project_name_from_query(query)
                if project_name:
                    logger.info(f"ğŸ¯ è‡ªåŠ¨æå–é¡¹ç›®åç§°: {project_name}")
            
            results = self.pdf_embedding_service.search_tables_only(
                query=query, 
                top_k=top_k, 
                source_file_filter=source_file_filter,
                project_name=project_name  # ğŸ†• é¡¹ç›®éš”ç¦»å‚æ•°
            )
            
            simplified_results = []
            for res in results:
                simplified_results.append({
                    "content": res.get("content", ""),
                    "metadata": res.get("metadata", {}),
                    "content_type": res.get("content_type", "table"),
                    "distance": res.get("distance", 0.0)
                })
            
            # ğŸ†• æ·»åŠ é¡¹ç›®éš”ç¦»ä¿¡æ¯åˆ°è¿”å›ç»“æœ
            result_data = {
                "status": "success", 
                "results": simplified_results,
                "total_count": len(simplified_results)
            }
            
            if project_name:
                result_data["project_isolation"] = {
                    "enabled": True,
                    "project_name": project_name,
                    "message": f"ğŸ”’ å·²é™åˆ¶æœç´¢èŒƒå›´è‡³é¡¹ç›®: {project_name}"
                }
            else:
                result_data["project_isolation"] = {
                    "enabled": False,
                    "message": "âš ï¸ æœªå¯ç”¨é¡¹ç›®éš”ç¦»ï¼Œæœç´¢äº†æ‰€æœ‰é¡¹ç›®"
                }
            
            return json.dumps(result_data, ensure_ascii=False)
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼æœç´¢å¤±è´¥: {e}")
            return json.dumps({"status": "error", "message": str(e)})
    
    def _count_images(self, query: str = "", source_file_filter: Optional[str] = None) -> str:
        """
        ç»Ÿè®¡å›¾ç‰‡æ•°é‡
        
        Args:
            query: æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
            source_file_filter: æºæ–‡ä»¶è¿‡æ»¤å™¨
            
        Returns:
            åŒ…å«ç»Ÿè®¡ç»“æœçš„JSONå­—ç¬¦ä¸²
        """
        if not self.pdf_embedding_service:
            return json.dumps({
                "status": "error", 
                "message": "PDF Embedding Service not available"
            })
        
        try:
            # å¦‚æœæœ‰æŸ¥è¯¢è¯ï¼Œè¿›è¡Œæœç´¢
            if query:
                results = self.pdf_embedding_service.search_images_only(
                    query=query, 
                    top_k=100,  # è·å–æ›´å¤šç»“æœæ¥ç»Ÿè®¡
                    source_file_filter=source_file_filter
                )
                count = len(results)
                message = f"æ‰¾åˆ° {count} å¼ åŒ…å«'{query}'çš„å›¾ç‰‡"
            else:
                # è·å–æ‰€æœ‰å›¾ç‰‡ç»Ÿè®¡
                stats = self.pdf_embedding_service.get_collection_stats()
                count = stats.get("image_embeddings", 0)
                message = f"ç³»ç»Ÿä¸­å…±æœ‰ {count} å¼ å›¾ç‰‡"
            
            return json.dumps({
                "status": "success", 
                "count": count,
                "message": message,
                "content_type": "image"
            }, ensure_ascii=False)
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡ç»Ÿè®¡å¤±è´¥: {e}")
            return json.dumps({"status": "error", "message": str(e)})
    
    def _count_tables(self, query: str = "", source_file_filter: Optional[str] = None) -> str:
        """
        ç»Ÿè®¡è¡¨æ ¼æ•°é‡
        
        Args:
            query: æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
            source_file_filter: æºæ–‡ä»¶è¿‡æ»¤å™¨
            
        Returns:
            åŒ…å«ç»Ÿè®¡ç»“æœçš„JSONå­—ç¬¦ä¸²
        """
        if not self.pdf_embedding_service:
            return json.dumps({
                "status": "error", 
                "message": "PDF Embedding Service not available"
            })
        
        try:
            # å¦‚æœæœ‰æŸ¥è¯¢è¯ï¼Œè¿›è¡Œæœç´¢
            if query:
                results = self.pdf_embedding_service.search_tables_only(
                    query=query, 
                    top_k=100,  # è·å–æ›´å¤šç»“æœæ¥ç»Ÿè®¡
                    source_file_filter=source_file_filter
                )
                count = len(results)
                message = f"æ‰¾åˆ° {count} ä¸ªåŒ…å«'{query}'çš„è¡¨æ ¼"
            else:
                # è·å–æ‰€æœ‰è¡¨æ ¼ç»Ÿè®¡
                stats = self.pdf_embedding_service.get_collection_stats()
                count = stats.get("table_embeddings", 0)
                message = f"ç³»ç»Ÿä¸­å…±æœ‰ {count} ä¸ªè¡¨æ ¼"
            
            return json.dumps({
                "status": "success", 
                "count": count,
                "message": message,
                "content_type": "table"
            }, ensure_ascii=False)
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼ç»Ÿè®¡å¤±è´¥: {e}")
            return json.dumps({"status": "error", "message": str(e)})
    
    def _fill_template_fields(self, template_fields_json: Dict[str, str]) -> str:
        """åŸºäºæ¨¡æ¿å­—æ®µJSONè¿›è¡Œæ™ºèƒ½å¡«å…… - æ ¸å¿ƒåŠŸèƒ½"""
        try:
            logger.info("ğŸ¯ å¼€å§‹æ¨¡æ¿å­—æ®µæ™ºèƒ½å¡«å……")
            
            # éªŒè¯è¾“å…¥
            if not isinstance(template_fields_json, dict):
                return "âŒ template_fields_json å¿…é¡»æ˜¯å­—å…¸æ ¼å¼"
            
            if not template_fields_json:
                return "âŒ template_fields_json ä¸èƒ½ä¸ºç©º"
            
            # æ‰§è¡Œæ™ºèƒ½å¡«å……
            filled_fields = self.field_processor.fill_template_fields(
                template_fields_json, self.vector_store
            )
            
            # æ ¼å¼åŒ–è¿”å›ç»“æœ
            result = f"âœ… æ¨¡æ¿å­—æ®µæ™ºèƒ½å¡«å……å®Œæˆï¼\n\n"
            result += f"ğŸ“‹ è¾“å…¥å­—æ®µ: {len(template_fields_json)} ä¸ª\n"
            result += f"ğŸ“ å¡«å……å­—æ®µ: {len(filled_fields)} ä¸ª\n\n"
            result += "ğŸ“„ å¡«å……ç»“æœ:\n"
            result += "=" * 50 + "\n"
            
            for field_name, filled_content in filled_fields.items():
                result += f"ğŸ”¸ {field_name}:\n"
                result += f"   {filled_content[:100]}{'...' if len(filled_content) > 100 else ''}\n\n"
            
            result += "=" * 50 + "\n"
            result += f"ğŸ’¾ å®Œæ•´å¡«å……ç»“æœJSON:\n{json.dumps(filled_fields, ensure_ascii=False, indent=2)}"
            
            return result
            
        except Exception as e:
            logger.error(f"æ¨¡æ¿å­—æ®µå¡«å……å¤±è´¥: {e}")
            return f"âŒ æ¨¡æ¿å­—æ®µå¡«å……å¤±è´¥: {str(e)}"
    
    def _list_documents(self) -> str:
        """åˆ—å‡ºæ‰€æœ‰å·²ä¸Šä¼ çš„æ–‡æ¡£"""
        try:
            documents = self.vector_store.get_all_documents()
            
            if not documents:
                return "ğŸ“š å½“å‰æ²¡æœ‰å·²ä¸Šä¼ çš„æ–‡æ¡£"
            
            result = f"ğŸ“š å·²ä¸Šä¼ æ–‡æ¡£åˆ—è¡¨ (å…± {len(documents)} ä¸ª):\n\n"
            
            for i, doc in enumerate(documents, 1):
                result += f"ğŸ“„ æ–‡æ¡£ {i}:\n"
                result += f"   ğŸ“ æ–‡ä»¶å: {doc['metadata'].get('filename', 'æœªçŸ¥')}\n"
                result += f"   ğŸ†” æ–‡æ¡£ID: {doc['id']}\n"
                result += f"   ğŸ“Š åˆ†å—æ•°: {doc['chunks']} ä¸ª\n"
                result += f"   â° ä¸Šä¼ æ—¶é—´: {doc['metadata'].get('upload_time', 'æœªçŸ¥')}\n\n"
            
            return result
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
            return f"âŒ è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}"
    
    def _clear_documents(self) -> str:
        """æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£"""
        try:
            self.vector_store.clear_all()
            return "âœ… æ‰€æœ‰æ–‡æ¡£å·²æˆåŠŸæ¸…ç©ºã€‚"
        except Exception as e:
            return f"âŒ æ¸…ç©ºæ–‡æ¡£å¤±è´¥: {str(e)}"
    
    def _process_parsed_folder(self, folder_path: str, project_name: str = "") -> str:
        """
        å¤„ç†PDFè§£æå·¥å…·ç”Ÿæˆçš„æ–‡ä»¶å¤¹ï¼Œå°†å…¶ä¸­çš„æ–‡æœ¬å†…å®¹æ·»åŠ åˆ°RAGçŸ¥è¯†åº“
        """
        try:
            content_file = os.path.join(folder_path, "parsed_content.json")
            if not os.path.exists(content_file):
                return f"âŒ 'parsed_content.json' not found in {folder_path}"
                
            with open(content_file, 'r', encoding='utf-8') as f:
                parsed_data = json.load(f)
            
            # æå–å…ƒæ•°æ®
            source_pdf_path = parsed_data.get("meta", {}).get("source_file", "æœªçŸ¥æ¥æº")
            doc_title = parsed_data.get("meta", {}).get("title", os.path.basename(folder_path))

            # å‡†å¤‡è¦å­˜å…¥çš„æ–‡æœ¬å—å’Œå…ƒæ•°æ®
            chunks = []
            metadatas = []
            
            # ä½¿ç”¨sectionsä½œä¸ºæ–‡æœ¬å—
            for section in parsed_data.get("sections", []):
                content = section.get("content")
                if not content:
                    continue
                
                chunks.append(content)
                
                # ä¸ºæ¯ä¸ªå—å‡†å¤‡å…ƒæ•°æ®
                chunk_metadata = {
                    "source_file": source_pdf_path,
                    "document_title": doc_title,
                    "project_name": project_name,
                    "section_title": section.get("title", ""),
                    "source_page": section.get("source_page", 0)
                }
                metadatas.append(chunk_metadata)

            if not chunks:
                return "âœ… æ–‡ä»¶å¤¹å¤„ç†å®Œæˆï¼Œæ²¡æœ‰æ‰¾åˆ°å¯ä¾›embeddingçš„æ–‡æœ¬å†…å®¹ã€‚"

            # æ‰¹é‡æ·»åŠ åˆ°å‘é‡åº“
            total_chunks_added = 0
            doc_id_prefix = hashlib.md5(source_pdf_path.encode()).hexdigest()

            for i, chunk in enumerate(chunks):
                doc_id = f"{doc_id_prefix}_section_{i}"
                self.vector_store.collection.add(
                    ids=[doc_id],
                    documents=[chunk],
                    metadatas=[metadatas[i]]
                )
                total_chunks_added += 1

            return f"âœ… æˆåŠŸå¤„ç†æ–‡ä»¶å¤¹ '{folder_path}'ï¼Œå·²æ·»åŠ  {total_chunks_added} ä¸ªæ–‡æœ¬å—åˆ°çŸ¥è¯†åº“ï¼Œæ¥æº: {os.path.basename(source_pdf_path)}."

        except Exception as e:
            logger.error(f"å¤„ç†è§£ææ–‡ä»¶å¤¹å¤±è´¥: {e}", exc_info=True)
            return f"âŒ å¤„ç†è§£ææ–‡ä»¶å¤¹å¤±è´¥: {str(e)}"
    
    def _extract_text_from_parsed_data(self, parsed_data: Dict[str, Any]) -> str:
        """ä»parsed_content.jsonä¸­æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼ˆä¿ç•™å¤‡ç”¨ï¼‰"""
        text_parts = []
        
        try:
            # å¤„ç†ä¸åŒçš„è§£ææ•°æ®ç»“æ„
            if isinstance(parsed_data, dict):
                # é€’å½’æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
                self._extract_text_recursive(parsed_data, text_parts)
            elif isinstance(parsed_data, list):
                for item in parsed_data:
                    if isinstance(item, dict):
                        self._extract_text_recursive(item, text_parts)
                    elif isinstance(item, str):
                        text_parts.append(item)
            elif isinstance(parsed_data, str):
                text_parts.append(parsed_data)
            
            # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
            full_text = "\n\n".join(text_parts)
            return full_text.strip()
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬æå–å¤±è´¥: {e}")
            return ""
    
    def _extract_text_recursive(self, data: Dict[str, Any], text_parts: List[str]):
        """
        é€’å½’æå–å­—å…¸ä¸­çš„æ–‡æœ¬å†…å®¹
        """
        for key, value in data.items():
            if isinstance(value, str) and len(value.strip()) > 0:
                # æ·»åŠ æœ‰æ„ä¹‰çš„æ–‡æœ¬å†…å®¹
                if len(value.strip()) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬
                    text_parts.append(f"[{key}]: {value.strip()}")
            elif isinstance(value, dict):
                # é€’å½’å¤„ç†åµŒå¥—å­—å…¸
                self._extract_text_recursive(value, text_parts)
            elif isinstance(value, list):
                # å¤„ç†åˆ—è¡¨
                for i, item in enumerate(value):
                    if isinstance(item, str) and len(item.strip()) > 10:
                        text_parts.append(f"[{key}_{i}]: {item.strip()}")
                    elif isinstance(item, dict):
                        self._extract_text_recursive(item, text_parts) 